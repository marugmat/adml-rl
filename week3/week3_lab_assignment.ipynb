{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Lab Assignment - REINFORCE for CartPole\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab, you will:\n",
    "1. Understand the policy network architecture\n",
    "2. Implement action selection from a stochastic policy\n",
    "3. Compute discounted returns\n",
    "4. Implement the REINFORCE policy gradient loss\n",
    "5. (Optional) Add a baseline for variance reduction\n",
    "\n",
    "## The CartPole Task\n",
    "\n",
    "**Goal:** Balance a pole on a moving cart for as long as possible\n",
    "\n",
    "**State (4 values):**\n",
    "- Cart position\n",
    "- Cart velocity\n",
    "- Pole angle\n",
    "- Pole angular velocity\n",
    "\n",
    "**Actions (2 discrete):**\n",
    "- 0: Push left\n",
    "- 1: Push right\n",
    "\n",
    "**Reward:** +1 for each timestep the pole stays up\n",
    "\n",
    "**Success:** Average reward ‚â• 475 over 100 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: Understanding the Policy Network\n",
    "\n",
    "The policy network maps states to action probabilities.\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "State (4) ‚Üí Hidden Layer (32) ‚Üí Action Logits (2)\n",
    "              with ReLU             then Softmax\n",
    "```\n",
    "\n",
    "### Task 1.1: Complete the Network Architecture\n",
    "\n",
    "Fill in the missing activation function and hidden dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Neural network policy for CartPole.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim=4, action_dim=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ============================================\n",
    "        # TODO: Set the hidden layer dimension\n",
    "        # Suggested: 32 neurons\n",
    "        # ============================================\n",
    "        hidden_dim = None  # Replace with a number\n",
    "        \n",
    "        # ============================================\n",
    "        # TODO: Fill in the activation function\n",
    "        # Use nn.ReLU()\n",
    "        # ============================================\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            None,  # Replace with activation function\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward pass: state ‚Üí action logits.\"\"\"\n",
    "        return self.network(state)\n",
    "\n",
    "\n",
    "# Test your network\n",
    "policy = PolicyNetwork()\n",
    "test_state = torch.FloatTensor([0.0, 0.0, 0.1, 0.0])\n",
    "logits = policy(test_state)\n",
    "\n",
    "print(f\"Input state shape: {test_state.shape}\")\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "print(f\"Output logits: {logits}\")\n",
    "\n",
    "if logits.shape[0] == 2:\n",
    "    print(\"\\n‚úì Network architecture looks correct!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Check your network architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Action Selection\n",
    "\n",
    "The policy must:\n",
    "1. Convert logits to probabilities using softmax\n",
    "2. Sample an action from the probability distribution\n",
    "3. Return the action and its log probability (for gradient computation)\n",
    "\n",
    "### Task 2.1: Implement Action Sampling\n",
    "\n",
    "Complete the `select_action` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Neural network policy for CartPole.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim=4, action_dim=2):\n",
    "        super().__init__()\n",
    "        hidden_dim = 32\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward pass: state ‚Üí action logits.\"\"\"\n",
    "        return self.network(state)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Sample an action from the policy.\n",
    "        \n",
    "        Args:\n",
    "            state: numpy array of shape (state_dim,)\n",
    "        \n",
    "        Returns:\n",
    "            action: integer (0 or 1)\n",
    "            log_prob: tensor containing log probability of the action\n",
    "        \"\"\"\n",
    "        # Convert state to tensor\n",
    "        state = torch.FloatTensor(state)\n",
    "        \n",
    "        # Get action logits from network\n",
    "        logits = self.forward(state)\n",
    "        \n",
    "        # ============================================\n",
    "        # TODO: Convert logits to probabilities\n",
    "        # Use torch.softmax(logits, dim=-1)\n",
    "        # ============================================\n",
    "        probs = None  # Replace with softmax of logits\n",
    "        \n",
    "        # ============================================\n",
    "        # TODO: Sample an action from the probability distribution\n",
    "        # Use torch.multinomial(probs, 1).item()\n",
    "        # ============================================\n",
    "        action = None  # Replace with sampled action\n",
    "        \n",
    "        # Compute log probability of the selected action\n",
    "        log_prob = torch.log(probs[action])\n",
    "        \n",
    "        return action, log_prob\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "policy = PolicyNetwork()\n",
    "test_state = np.array([0.0, 0.0, 0.1, 0.0])\n",
    "\n",
    "print(\"Testing action selection:\")\n",
    "for i in range(5):\n",
    "    action, log_prob = policy.select_action(test_state)\n",
    "    print(f\"  Trial {i+1}: action={action}, log_prob={log_prob.item():.3f}\")\n",
    "\n",
    "print(\"\\n‚úì If you see actions (0 or 1) and log probabilities (negative numbers), it's working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 3: Computing Discounted Returns\n",
    "\n",
    "The return G_t is the total discounted reward from timestep t:\n",
    "\n",
    "$$G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... = \\sum_{k=0}^{T-t} \\gamma^k r_{t+k}$$\n",
    "\n",
    "**Efficient computation:** Work backwards from the end:\n",
    "```\n",
    "G_T = r_T\n",
    "G_{T-1} = r_{T-1} + Œ≥ * G_T\n",
    "G_{T-2} = r_{T-2} + Œ≥ * G_{T-1}\n",
    "...\n",
    "```\n",
    "\n",
    "### Task 3.1: Implement the Return Calculation\n",
    "\n",
    "This is the **key concept** of REINFORCE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute discounted returns for each timestep.\n",
    "    \n",
    "    Args:\n",
    "        rewards: list of rewards [r_0, r_1, ..., r_T]\n",
    "        gamma: discount factor\n",
    "    \n",
    "    Returns:\n",
    "        returns: tensor of returns [G_0, G_1, ..., G_T]\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    R = 0  # Running return\n",
    "    \n",
    "    # ============================================\n",
    "    # TODO: Compute returns by iterating backwards through rewards\n",
    "    # For each reward r (starting from the end):\n",
    "    #   1. Update R = r + gamma * R\n",
    "    #   2. Insert R at the beginning of returns list\n",
    "    # \n",
    "    # Hint: Use reversed(rewards) to iterate backwards\n",
    "    # Hint: Use returns.insert(0, R) to insert at beginning\n",
    "    # ============================================\n",
    "    \n",
    "    # Your code here (2-3 lines)\n",
    "    \n",
    "    \n",
    "    return torch.FloatTensor(returns)\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "test_rewards = [1, 1, 1, 1, 1]  # 5 timesteps, reward=1 each\n",
    "test_gamma = 0.9\n",
    "\n",
    "returns = compute_returns(test_rewards, test_gamma)\n",
    "\n",
    "print(f\"Rewards: {test_rewards}\")\n",
    "print(f\"Gamma: {test_gamma}\")\n",
    "print(f\"\\nComputed returns: {returns}\")\n",
    "\n",
    "# Expected (working backwards):\n",
    "# G_4 = 1\n",
    "# G_3 = 1 + 0.9 * 1 = 1.9\n",
    "# G_2 = 1 + 0.9 * 1.9 = 2.71\n",
    "# G_1 = 1 + 0.9 * 2.71 = 3.439\n",
    "# G_0 = 1 + 0.9 * 3.439 = 4.0951\n",
    "\n",
    "expected = torch.FloatTensor([4.0951, 3.439, 2.71, 1.9, 1.0])\n",
    "if torch.allclose(returns, expected, atol=0.01):\n",
    "    print(\"\\n‚úì Correct! Your returns match the expected values.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Expected approximately: {expected}\")\n",
    "    print(\"Check your loop implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 4: Policy Gradient Loss\n",
    "\n",
    "The REINFORCE loss is:\n",
    "\n",
    "$$L = -\\sum_{t=0}^T \\log \\pi_\\theta(a_t|s_t) \\cdot G_t$$\n",
    "\n",
    "**Intuition:**\n",
    "- If G_t is high (good outcome) ‚Üí increase log œÄ(a_t|s_t) ‚Üí increase P(a_t|s_t)\n",
    "- If G_t is low (bad outcome) ‚Üí decrease log œÄ(a_t|s_t) ‚Üí decrease P(a_t|s_t)\n",
    "- The negative sign is because we minimize loss (= maximize reward)\n",
    "\n",
    "### Task 4.1: Implement the Loss Function\n",
    "\n",
    "This connects actions to outcomes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_policy_loss(log_probs, returns):\n",
    "    \"\"\"\n",
    "    Compute REINFORCE policy gradient loss.\n",
    "    \n",
    "    Args:\n",
    "        log_probs: list of log probabilities for each action\n",
    "        returns: tensor of returns for each timestep\n",
    "    \n",
    "    Returns:\n",
    "        loss: scalar tensor\n",
    "    \"\"\"\n",
    "    # ============================================\n",
    "    # TODO: Compute the policy gradient loss\n",
    "    # \n",
    "    # For each timestep t:\n",
    "    #   loss += -log_probs[t] * returns[t]\n",
    "    # \n",
    "    # Hint: You can use a loop, or use:\n",
    "    #   torch.stack(log_probs) to convert list to tensor\n",
    "    #   then multiply element-wise and sum\n",
    "    # ============================================\n",
    "    \n",
    "    loss = 0\n",
    "    # Your code here (1-3 lines)\n",
    "    \n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "test_log_probs = [\n",
    "    torch.tensor(-0.5),\n",
    "    torch.tensor(-0.7),\n",
    "    torch.tensor(-0.6)\n",
    "]\n",
    "test_returns = torch.FloatTensor([3.0, 2.0, 1.0])\n",
    "\n",
    "loss = compute_policy_loss(test_log_probs, test_returns)\n",
    "\n",
    "print(f\"Log probs: {[lp.item() for lp in test_log_probs]}\")\n",
    "print(f\"Returns: {test_returns.tolist()}\")\n",
    "print(f\"\\nComputed loss: {loss.item():.3f}\")\n",
    "\n",
    "# Expected: -(-0.5*3.0 + -0.7*2.0 + -0.6*1.0) = -(-1.5 - 1.4 - 0.6) = 3.5\n",
    "expected_loss = 3.5\n",
    "if abs(loss.item() - expected_loss) < 0.01:\n",
    "    print(f\"\\n‚úì Correct! Loss = {expected_loss}\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Expected loss ‚âà {expected_loss}\")\n",
    "    print(\"Check your loss calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Putting It All Together: Training Loop\n",
    "\n",
    "Now we'll use your implementations to train the agent!\n",
    "\n",
    "The training loop is provided - it uses all the functions you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce(env, policy, optimizer, n_episodes=1000, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Train policy using REINFORCE algorithm.\n",
    "    \n",
    "    This function uses your implementations of:\n",
    "    - policy.select_action()\n",
    "    - compute_returns()\n",
    "    - compute_policy_loss()\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        # Sample an episode\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        \n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action using your implementation\n",
    "            action, log_prob = policy.select_action(state)\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "        \n",
    "        # Compute returns using your implementation\n",
    "        returns = compute_returns(rewards, gamma)\n",
    "        \n",
    "        # Normalize returns (variance reduction)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Compute loss using your implementation\n",
    "        loss = compute_policy_loss(log_probs, returns)\n",
    "        \n",
    "        # Update policy\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track performance\n",
    "        episode_rewards.append(sum(rewards))\n",
    "        \n",
    "        # Print progress\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            print(f\"Episode {episode + 1}: Avg Reward = {avg_reward:.2f}\")\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "# Create environment and policy\n",
    "env = gym.make('CartPole-v1')\n",
    "policy = PolicyNetwork()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Training REINFORCE on CartPole-v1...\\n\")\n",
    "rewards = train_reinforce(env, policy, optimizer, n_episodes=1000)\n",
    "\n",
    "print(f\"\\nFinal 100-episode average: {np.mean(rewards[-100:]):.2f}\")\n",
    "if np.mean(rewards[-100:]) >= 475:\n",
    "    print(\"üéâ SUCCESS! Agent has solved CartPole!\")\n",
    "else:\n",
    "    print(\"Keep training or try adjusting hyperparameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Raw rewards\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards, alpha=0.3, color='blue', label='Episode reward')\n",
    "\n",
    "# Moving average\n",
    "window = 50\n",
    "if len(rewards) >= window:\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    plt.plot(range(window-1, len(rewards)), moving_avg, color='red', linewidth=2, label=f'{window}-episode MA')\n",
    "\n",
    "plt.axhline(y=475, color='g', linestyle='--', alpha=0.7, label='Success threshold')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Reward distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "early_rewards = rewards[:min(200, len(rewards)//2)]\n",
    "late_rewards = rewards[max(len(rewards)//2, len(rewards)-200):]\n",
    "\n",
    "plt.hist(early_rewards, bins=20, alpha=0.5, label=f'Early episodes')\n",
    "plt.hist(late_rewards, bins=20, alpha=0.5, label=f'Late episodes')\n",
    "plt.xlabel('Episode Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Reward Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualizing the Trained Agent\n",
    "\n",
    "Let's watch your trained agent in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_episode(env, policy, max_steps=500):\n",
    "    \"\"\"Record and display an episode.\"\"\"\n",
    "    frames = []\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Render\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        \n",
    "        # Select action (greedy)\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        with torch.no_grad():\n",
    "            logits = policy(state_tensor)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            action = torch.argmax(probs).item()\n",
    "        \n",
    "        # Step\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    print(f\"Episode lasted {step + 1} steps, total reward: {total_reward}\")\n",
    "    \n",
    "    # Create animation\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.axis('off')\n",
    "    img = ax.imshow(frames[0])\n",
    "    \n",
    "    def animate(i):\n",
    "        img.set_data(frames[i])\n",
    "        return [img]\n",
    "    \n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(frames), interval=20, blit=True)\n",
    "    plt.close()\n",
    "    \n",
    "    return HTML(anim.to_html5_video())\n",
    "\n",
    "\n",
    "# Display trained policy\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "display_episode(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 5 (Optional): Add a Better Baseline\n",
    "\n",
    "We used return normalization (subtracting mean) as a simple baseline. This reduces variance but we can do better!\n",
    "\n",
    "**Better baseline:** Use a value network V(s) that estimates the expected return from each state.\n",
    "\n",
    "**New loss:**\n",
    "$$L = -\\sum_{t=0}^T \\log \\pi_\\theta(a_t|s_t) \\cdot (G_t - V_\\phi(s_t))$$\n",
    "\n",
    "This is a preview of **Actor-Critic** (next week)!\n",
    "\n",
    "### Task 5.1: Implement a Value Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"Value network for estimating V(s).\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim=4, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ============================================\n",
    "        # TODO: Create a network that outputs a single value\n",
    "        # Architecture: state_dim ‚Üí hidden_dim (ReLU) ‚Üí 1\n",
    "        # ============================================\n",
    "        self.network = nn.Sequential(\n",
    "            # Your code here\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Estimate value of state.\"\"\"\n",
    "        return self.network(state).squeeze()\n",
    "\n",
    "\n",
    "def train_with_value_baseline(env, policy, value_net, policy_opt, value_opt, n_episodes=1000, gamma=0.99):\n",
    "    \"\"\"Train with learned value baseline.\"\"\"\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        states = []\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        values = []\n",
    "        \n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            \n",
    "            # ============================================\n",
    "            # TODO: Get value estimate for current state\n",
    "            # ============================================\n",
    "            value = None  # value_net(state_tensor)\n",
    "            \n",
    "            action, log_prob = policy.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            states.append(state_tensor)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "            state = next_state\n",
    "        \n",
    "        # Compute returns\n",
    "        returns = compute_returns(rewards, gamma)\n",
    "        \n",
    "        # ============================================\n",
    "        # TODO: Compute advantages (returns - values)\n",
    "        # ============================================\n",
    "        values_tensor = torch.stack(values)\n",
    "        advantages = None  # returns - values_tensor.detach()\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # ============================================\n",
    "        # TODO: Update policy using advantages instead of returns\n",
    "        # ============================================\n",
    "        policy_loss = compute_policy_loss(log_probs, advantages)\n",
    "        \n",
    "        policy_opt.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_opt.step()\n",
    "        \n",
    "        # ============================================\n",
    "        # TODO: Update value network to predict returns better\n",
    "        # Use MSE loss: (values - returns)^2\n",
    "        # ============================================\n",
    "        value_loss = None  # ((values_tensor - returns) ** 2).mean()\n",
    "        \n",
    "        value_opt.zero_grad()\n",
    "        value_loss.backward()\n",
    "        value_opt.step()\n",
    "        \n",
    "        episode_rewards.append(sum(rewards))\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            print(f\"Episode {episode + 1}: Avg Reward = {avg_reward:.2f}\")\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "# Uncomment to train with value baseline:\n",
    "# env = gym.make('CartPole-v1')\n",
    "# policy = PolicyNetwork()\n",
    "# value_net = ValueNetwork()\n",
    "# policy_opt = optim.Adam(policy.parameters(), lr=0.01)\n",
    "# value_opt = optim.Adam(value_net.parameters(), lr=0.01)\n",
    "\n",
    "# print(\"Training with value baseline...\\n\")\n",
    "# rewards_baseline = train_with_value_baseline(env, policy, value_net, policy_opt, value_opt, n_episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully:\n",
    "- Built a policy network\n",
    "- Implemented action sampling\n",
    "- Computed discounted returns\n",
    "- Implemented the REINFORCE algorithm\n",
    "- Trained an agent to solve CartPole!\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Policy gradients** directly optimize the policy œÄ_Œ∏(a|s)\n",
    "2. **Returns** assign credit to actions based on future outcomes\n",
    "3. **Stochastic policies** are crucial for exploration\n",
    "4. **Variance reduction** (baselines) improves learning stability\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "**Next week:** Actor-Critic methods\n",
    "- Combine policy gradient with value function\n",
    "- Bootstrap instead of Monte Carlo\n",
    "- Lower variance, faster learning\n",
    "\n",
    "**Try at home:**\n",
    "- Run the standalone Python script (see README)\n",
    "- Experiment with hyperparameters\n",
    "- Try other gym environments (MountainCar, LunarLander)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
