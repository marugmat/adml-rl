{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Policy Gradient Methods\n",
    "## Lecture Demonstration\n",
    "\n",
    "**Course:** Reinforcement Learning - Continuing Education  \n",
    "**Institution:** Zurich University of Applied Sciences\n",
    "\n",
    "---\n",
    "\n",
    "## Today's Topics\n",
    "1. From Value-Based to Policy-Based Methods\n",
    "2. The Policy Gradient Theorem\n",
    "3. REINFORCE Algorithm\n",
    "4. Variance Reduction with Baselines\n",
    "5. Live Demo: CartPole with Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Recap: Value-Based vs Policy-Based\n",
    "\n",
    "### Value-Based (Q-Learning)\n",
    "- Learn Q(s,a) for all state-action pairs\n",
    "- Policy is **implicit**: π(s) = argmax_a Q(s,a)\n",
    "- Works well for discrete actions\n",
    "- Policy is always deterministic\n",
    "\n",
    "### Policy-Based (Today)\n",
    "- Learn policy **directly**: π_θ(a|s)\n",
    "- No need for value function (though we can add one later!)\n",
    "- Can handle continuous actions naturally\n",
    "- Can learn stochastic policies\n",
    "\n",
    "### Why Policy Gradient?\n",
    "1. **Continuous actions**: Can't do argmax over infinite actions\n",
    "2. **Stochastic policies**: Sometimes randomness is optimal (e.g., rock-paper-scissors)\n",
    "3. **Smoother optimization**: Small parameter changes → small policy changes\n",
    "4. **Convergence guarantees**: Under certain conditions, guaranteed to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Policy Gradient Theorem\n",
    "\n",
    "### Goal\n",
    "Maximize expected return:\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^T \\gamma^t r_t\\right]$$\n",
    "\n",
    "Where τ (tau) is a trajectory: (s₀, a₀, r₀, s₁, a₁, r₁, ...)\n",
    "\n",
    "### The Key Insight\n",
    "We can't differentiate through the environment (stochastic!), but we can use the **log-derivative trick**:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot G_t\\right]$$\n",
    "\n",
    "Where:\n",
    "- $G_t = \\sum_{k=t}^T \\gamma^{k-t} r_k$ is the return from time t\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)$ is the gradient of log probability\n",
    "\n",
    "### Intuition\n",
    "- If action led to high return → increase its probability\n",
    "- If action led to low return → decrease its probability\n",
    "- The gradient tells us which direction to adjust parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. REINFORCE Algorithm\n",
    "\n",
    "**Monte Carlo Policy Gradient** (Williams, 1992)\n",
    "\n",
    "```\n",
    "Initialize policy network π_θ with random weights θ\n",
    "\n",
    "for episode = 1 to N:\n",
    "    Generate episode: s₀, a₀, r₀, ..., s_T, a_T, r_T using π_θ\n",
    "    \n",
    "    for t = 0 to T:\n",
    "        Compute return: G_t = Σ γ^k r_{t+k}\n",
    "        \n",
    "    Compute loss: L = -Σ log π_θ(a_t|s_t) * G_t\n",
    "    \n",
    "    Update θ using gradient descent on L\n",
    "```\n",
    "\n",
    "### Why the Negative Sign?\n",
    "- We want to **maximize** J(θ)\n",
    "- Optimizers **minimize** loss\n",
    "- So we minimize -J(θ), which is the same as maximizing J(θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Toy Example: 2-State MDP\n",
    "\n",
    "Let's see policy gradients on the simplest possible problem.\n",
    "\n",
    "**Setup:**\n",
    "- 2 states: {0, 1}\n",
    "- 2 actions: {left, right}\n",
    "- Start in state 0\n",
    "- Reward: +1 if you reach state 1, 0 otherwise\n",
    "- Episode ends when you reach state 1\n",
    "\n",
    "**Optimal policy:** Always go right from state 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyPolicyNetwork(nn.Module):\n",
    "    \"\"\"Simplest possible policy network.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Just a single parameter: logit for \"right\" action\n",
    "        self.logit = nn.Parameter(torch.tensor([0.0]))\n",
    "    \n",
    "    def forward(self, state):\n",
    "        # Probability of going right\n",
    "        prob_right = torch.sigmoid(self.logit)\n",
    "        return torch.stack([1 - prob_right, prob_right])\n",
    "\n",
    "\n",
    "def toy_environment(action):\n",
    "    \"\"\"Simple 2-state MDP.\"\"\"\n",
    "    if action == 1:  # right\n",
    "        return 1, 1.0, True  # next_state, reward, done\n",
    "    else:  # left\n",
    "        return 0, 0.0, False  # stay in state 0, no reward\n",
    "\n",
    "\n",
    "# Train the tiny policy\n",
    "policy = TinyPolicyNetwork()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.1)\n",
    "\n",
    "logits_history = []\n",
    "prob_right_history = []\n",
    "\n",
    "for episode in range(100):\n",
    "    # Sample action\n",
    "    probs = policy(None)\n",
    "    action = torch.multinomial(probs, 1).item()\n",
    "    \n",
    "    # Take action\n",
    "    next_state, reward, done = toy_environment(action)\n",
    "    \n",
    "    # Compute loss\n",
    "    log_prob = torch.log(probs[action])\n",
    "    loss = -log_prob * reward  # Policy gradient\n",
    "    \n",
    "    # Update\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Track\n",
    "    logits_history.append(policy.logit.item())\n",
    "    prob_right_history.append(torch.sigmoid(policy.logit).item())\n",
    "\n",
    "# Plot learning\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(logits_history)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Logit (raw parameter)')\n",
    "ax1.set_title('Policy Parameter Over Time')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(prob_right_history)\n",
    "ax2.axhline(y=1.0, color='g', linestyle='--', label='Optimal (100%)')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('P(right)')\n",
    "ax2.set_title('Probability of Going Right')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal probability of going right: {prob_right_history[-1]:.3f}\")\n",
    "print(\"(Optimal: 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** The policy learns to go right (optimal!) through trial and error.\n",
    "\n",
    "Notice the **credit assignment**: When we go right and get reward=1, we increase P(right). When we go left and get reward=0, we don't change P(left) much (because gradient is near zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CartPole Environment\n",
    "\n",
    "Now let's tackle a real problem: **CartPole-v1**\n",
    "\n",
    "**Goal:** Balance a pole on a moving cart\n",
    "\n",
    "**State:** 4 continuous values\n",
    "- Cart position: [-4.8, 4.8]\n",
    "- Cart velocity: [-∞, ∞]\n",
    "- Pole angle: [-0.418, 0.418] rad (~24°)\n",
    "- Pole angular velocity: [-∞, ∞]\n",
    "\n",
    "**Actions:** 2 discrete\n",
    "- 0: Push cart left\n",
    "- 1: Push cart right\n",
    "\n",
    "**Reward:** +1 for every timestep the pole stays up\n",
    "\n",
    "**Episode ends if:**\n",
    "- Pole angle > 12°\n",
    "- Cart position > 2.4\n",
    "- 500 timesteps elapsed\n",
    "\n",
    "**Success criterion:** Average reward ≥ 475 over 100 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "print(\"CartPole-v1 Environment:\")\n",
    "print(f\"  State space: {env.observation_space}\")\n",
    "print(f\"  Action space: {env.action_space}\")\n",
    "print(f\"  Max episode steps: {env.spec.max_episode_steps}\")\n",
    "\n",
    "# Test random policy\n",
    "state, _ = env.reset(seed=42)\n",
    "print(f\"\\nInitial state: {state}\")\n",
    "print(f\"  [cart_pos, cart_vel, pole_angle, pole_ang_vel]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Policy Network for CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Neural network policy for CartPole.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim=4, hidden_dim=32, action_dim=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward pass: state → action logits.\"\"\"\n",
    "        return self.network(state)\n",
    "    \n",
    "    def get_action_probs(self, state):\n",
    "        \"\"\"Get action probabilities using softmax.\"\"\"\n",
    "        logits = self.forward(state)\n",
    "        return torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Sample action from policy and return (action, log_prob).\"\"\"\n",
    "        state = torch.FloatTensor(state)\n",
    "        probs = self.get_action_probs(state)\n",
    "        \n",
    "        # Sample action from categorical distribution\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        log_prob = torch.log(probs[action])\n",
    "        \n",
    "        return action, log_prob\n",
    "\n",
    "\n",
    "# Test the policy\n",
    "policy = PolicyNetwork()\n",
    "test_state = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "print(\"Testing policy network:\")\n",
    "for _ in range(3):\n",
    "    action, log_prob = policy.select_action(test_state)\n",
    "    print(f\"  Action: {action}, log P(a|s): {log_prob.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. REINFORCE Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute discounted returns for each timestep.\n",
    "    \n",
    "    G_t = r_t + γ*r_{t+1} + γ²*r_{t+2} + ...\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    R = 0\n",
    "    \n",
    "    # Work backwards from end of episode\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    \n",
    "    return torch.FloatTensor(returns)\n",
    "\n",
    "\n",
    "def train_reinforce(env, policy, optimizer, n_episodes=1000, gamma=0.99):\n",
    "    \"\"\"Train policy using REINFORCE algorithm.\"\"\"\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        # Sample episode\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        \n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action, log_prob = policy.select_action(state)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        # Compute returns\n",
    "        returns = compute_returns(rewards, gamma)\n",
    "        \n",
    "        # Normalize returns (variance reduction)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Compute policy gradient loss\n",
    "        loss = 0\n",
    "        for log_prob, G in zip(log_probs, returns):\n",
    "            loss += -log_prob * G\n",
    "        \n",
    "        # Update policy\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track performance\n",
    "        episode_rewards.append(sum(rewards))\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            print(f\"Episode {episode + 1}: Avg Reward = {avg_reward:.2f}\")\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "# Train the policy\n",
    "print(\"Training REINFORCE on CartPole-v1...\\n\")\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "policy = PolicyNetwork()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "\n",
    "rewards = train_reinforce(env, policy, optimizer, n_episodes=1000)\n",
    "\n",
    "print(f\"\\nFinal 100-episode average: {np.mean(rewards[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Raw rewards\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards, alpha=0.3, color='blue')\n",
    "# Moving average\n",
    "window = 50\n",
    "moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "plt.plot(range(window-1, len(rewards)), moving_avg, color='red', linewidth=2, label=f'{window}-episode MA')\n",
    "plt.axhline(y=475, color='g', linestyle='--', label='Success threshold')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.title('REINFORCE Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution of rewards\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(rewards[:200], bins=30, alpha=0.5, label='Episodes 0-200')\n",
    "plt.hist(rewards[-200:], bins=30, alpha=0.5, label='Episodes 800-1000')\n",
    "plt.xlabel('Episode Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Reward Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing the Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_episode(env, policy, max_steps=500):\n",
    "    \"\"\"Record episode and display as animation.\"\"\"\n",
    "    frames = []\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        # Render\n",
    "        frame = env.render()\n",
    "        frames.append(frame)\n",
    "        \n",
    "        # Select action (greedy, no exploration)\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        with torch.no_grad():\n",
    "            probs = policy.get_action_probs(state_tensor)\n",
    "            action = torch.argmax(probs).item()\n",
    "        \n",
    "        # Step\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # Create animation\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.axis('off')\n",
    "    img = ax.imshow(frames[0])\n",
    "    \n",
    "    def animate(i):\n",
    "        img.set_data(frames[i])\n",
    "        return [img]\n",
    "    \n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(frames), interval=20, blit=True)\n",
    "    plt.close()\n",
    "    \n",
    "    return HTML(anim.to_html5_video())\n",
    "\n",
    "\n",
    "# Display trained policy\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "print(\"Trained policy in action:\")\n",
    "display_episode(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Variance Reduction: Adding a Baseline\n",
    "\n",
    "### Problem\n",
    "Policy gradients have **high variance**:\n",
    "- Even good actions might get low returns due to randomness\n",
    "- This makes learning unstable and slow\n",
    "\n",
    "### Solution: Baseline\n",
    "Subtract a baseline b(s) from returns:\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}\\left[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot (G_t - b(s_t))\\right]$$\n",
    "\n",
    "**Key insight:** This doesn't change the expectation (bias), but reduces variance!\n",
    "\n",
    "**Common baseline:** Average return across the episode\n",
    "\n",
    "**Better baseline:** Value function V(s) → leads to Actor-Critic (next week!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already used return normalization above, which is a form of baseline!\n",
    "# Let's compare with and without:\n",
    "\n",
    "def train_comparison():\n",
    "    \"\"\"Compare with/without baseline.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for use_baseline in [False, True]:\n",
    "        env = gym.make('CartPole-v1')\n",
    "        policy = PolicyNetwork()\n",
    "        optimizer = optim.Adam(policy.parameters(), lr=0.01)\n",
    "        rewards = []\n",
    "        \n",
    "        for episode in range(500):\n",
    "            log_probs = []\n",
    "            episode_rewards = []\n",
    "            \n",
    "            state, _ = env.reset()\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action, log_prob = policy.select_action(state)\n",
    "                state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                log_probs.append(log_prob)\n",
    "                episode_rewards.append(reward)\n",
    "            \n",
    "            returns = compute_returns(episode_rewards, gamma=0.99)\n",
    "            \n",
    "            if use_baseline:\n",
    "                # Normalize returns (subtract mean, divide by std)\n",
    "                returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "            \n",
    "            loss = sum(-lp * G for lp, G in zip(log_probs, returns))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            rewards.append(sum(episode_rewards))\n",
    "        \n",
    "        results[use_baseline] = rewards\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Comparing with/without baseline...\")\n",
    "comparison = train_comparison()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for use_baseline, rewards in comparison.items():\n",
    "    window = 50\n",
    "    moving_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    label = 'With baseline' if use_baseline else 'Without baseline'\n",
    "    plt.plot(range(window-1, len(rewards)), moving_avg, label=label, linewidth=2)\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward (50-episode MA)')\n",
    "plt.title('Effect of Baseline on Learning')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBaseline typically leads to:\")\n",
    "print(\"  - Faster learning\")\n",
    "print(\"  - More stable training\")\n",
    "print(\"  - Better final performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Takeaways\n",
    "\n",
    "### REINFORCE Summary\n",
    "1. **Direct policy optimization**: Learn π_θ(a|s) directly\n",
    "2. **Monte Carlo**: Use full episode returns\n",
    "3. **Policy gradient theorem**: ∇J = E[∇log π * G]\n",
    "4. **Simple but effective**: Works for continuous and discrete actions\n",
    "\n",
    "### Advantages\n",
    "- Can learn stochastic policies\n",
    "- Works with continuous action spaces\n",
    "- Guaranteed convergence (to local optimum)\n",
    "- Simple to implement\n",
    "\n",
    "### Disadvantages\n",
    "- High variance → slow learning\n",
    "- Sample inefficient (on-policy, Monte Carlo)\n",
    "- Can get stuck in local optima\n",
    "- Sensitive to hyperparameters\n",
    "\n",
    "### Improvements (Next Steps)\n",
    "1. **Baselines**: Reduce variance → Actor-Critic\n",
    "2. **Bootstrapping**: Use TD instead of MC → A2C, A3C\n",
    "3. **Trust regions**: Constrain updates → PPO, TRPO\n",
    "4. **Off-policy**: Reuse data → DDPG, SAC\n",
    "\n",
    "---\n",
    "\n",
    "## Next: Lab Exercise\n",
    "You'll implement REINFORCE yourself and train an agent to solve CartPole!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
