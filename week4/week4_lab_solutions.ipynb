{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Solutions: Deep RL with LunarLander\n",
    "\n",
    "This notebook contains the complete lab code with expected results and additional insights.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable-baselines3[extra] gymnasium[box2d] -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Solution: Exploring the Environment\n",
    "\n",
    "**Expected output:**\n",
    "- Observation space: Box(8,)\n",
    "- Action space: Discrete(4)\n",
    "- Random policy mean reward: ~-150 to -250 (crashes!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "print(\"Environment Info:\")\n",
    "print(f\"  Observation space: {env.observation_space}\")\n",
    "print(f\"  Action space: {env.action_space}\")\n",
    "\n",
    "state, _ = env.reset(seed=42)\n",
    "print(f\"\\nInitial state: {state}\")\n",
    "\n",
    "\n",
    "def evaluate_random_policy(env, n_episodes=10):\n",
    "    episode_rewards = []\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "random_rewards = evaluate_random_policy(env, n_episodes=10)\n",
    "print(f\"\\nRandom Policy Performance:\")\n",
    "print(f\"  Mean: {np.mean(random_rewards):.2f}\")\n",
    "print(f\"  Std: {np.std(random_rewards):.2f}\")\n",
    "print(f\"\\nâœ“ Expected: Mean around -150 to -250 (random agents crash)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 Solution: Training PPO\n",
    "\n",
    "**Expected results after 100k steps:**\n",
    "- Mean reward: 180-240 (depends on random seed)\n",
    "- Training time: ~3-5 minutes\n",
    "- Should be close to solving (200+) or already solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "model_ppo = PPO(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Training PPO...\\n\")\n",
    "start = time.time()\n",
    "model_ppo.learn(total_timesteps=100_000)\n",
    "train_time = time.time() - start\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model_ppo, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"PPO Results:\")\n",
    "print(f\"  Training time: {train_time:.1f}s\")\n",
    "print(f\"  Mean reward: {mean_reward:.2f} Â± {std_reward:.2f}\")\n",
    "print(f\"  Solved: {'âœ“' if mean_reward >= 200 else 'âœ—'}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"\\nâœ“ Expected: 180-240 mean reward\")\n",
    "print(f\"  If <200: Train longer or try different seed\")\n",
    "print(f\"  If >200: Great! PPO solved it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Solution: Algorithm Comparison\n",
    "\n",
    "**Expected performance rankings (after 100k steps):**\n",
    "1. **PPO**: 180-240 (most reliable)\n",
    "2. **DQN**: 150-220 (can be unstable)\n",
    "3. **A2C**: 120-200 (faster but higher variance)\n",
    "4. **Random**: -150 to -250 (baseline)\n",
    "\n",
    "**Note:** Exact values depend on random seed, but relative ranking usually holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train A2C\n",
    "print(\"Training A2C...\\n\")\n",
    "env = gym.make('LunarLander-v2')\n",
    "model_a2c = A2C('MlpPolicy', env, learning_rate=7e-4, verbose=1, seed=42)\n",
    "model_a2c.learn(total_timesteps=100_000)\n",
    "mean_a2c, std_a2c = evaluate_policy(model_a2c, env, n_eval_episodes=100)\n",
    "print(f\"A2C: {mean_a2c:.2f} Â± {std_a2c:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DQN\n",
    "print(\"\\nTraining DQN...\\n\")\n",
    "env = gym.make('LunarLander-v2')\n",
    "model_dqn = DQN('MlpPolicy', env, learning_rate=1e-4, verbose=1, seed=42)\n",
    "model_dqn.learn(total_timesteps=100_000)\n",
    "mean_dqn, std_dqn = evaluate_policy(model_dqn, env, n_eval_episodes=100)\n",
    "print(f\"DQN: {mean_dqn:.2f} Â± {std_dqn:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison plot\n",
    "algorithms = ['Random', 'PPO', 'A2C', 'DQN']\n",
    "mean_rewards = [np.mean(random_rewards), mean_reward, mean_a2c, mean_dqn]\n",
    "std_rewards = [np.std(random_rewards), std_reward, std_a2c, std_dqn]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "bars = plt.bar(algorithms, mean_rewards, yerr=std_rewards, \n",
    "               capsize=5, color=colors, alpha=0.7)\n",
    "plt.axhline(y=200, color='black', linestyle='--', label='Solved (200)')\n",
    "plt.ylabel('Mean Reward')\n",
    "plt.title('Algorithm Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, mean in zip(bars, mean_rewards):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "             f'{mean:.0f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Typical observations:\")\n",
    "print(\"  - PPO usually performs best (190-240)\")\n",
    "print(\"  - A2C is faster to train but more variable (120-200)\")\n",
    "print(\"  - DQN needs careful tuning (150-220)\")\n",
    "print(\"  - All beat random (-200) by a large margin!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 Solution: Hyperparameter Experiments\n",
    "\n",
    "**Expected results for learning rate experiments:**\n",
    "- **LR=1e-4 (low)**: 100-160 (learns slowly, may not solve)\n",
    "- **LR=3e-4 (default)**: 180-220 (good balance)\n",
    "- **LR=1e-3 (high)**: 140-200 (can be unstable)\n",
    "\n",
    "**Key insight:** Default learning rate (3e-4) is usually best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(learning_rate, timesteps=50_000):\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    model = PPO('MlpPolicy', env, learning_rate=learning_rate, \n",
    "                verbose=0, seed=42)\n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    mean, std = evaluate_policy(model, env, n_eval_episodes=50)\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "learning_rates = [1e-4, 3e-4, 1e-3]\n",
    "results = {}\n",
    "\n",
    "print(\"Testing different learning rates...\\n\")\n",
    "for lr in learning_rates:\n",
    "    print(f\"LR={lr:.0e}: \", end=\"\")\n",
    "    mean, std = train_and_evaluate(lr)\n",
    "    results[lr] = (mean, std)\n",
    "    print(f\"{mean:.2f} Â± {std:.2f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "lrs = [f\"{lr:.0e}\" for lr in learning_rates]\n",
    "means = [results[lr][0] for lr in learning_rates]\n",
    "stds = [results[lr][1] for lr in learning_rates]\n",
    "\n",
    "plt.bar(lrs, means, yerr=stds, capsize=5, alpha=0.7, color='steelblue')\n",
    "plt.axhline(y=200, color='g', linestyle='--', label='Solved')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Mean Reward')\n",
    "plt.title('Impact of Learning Rate (50k steps)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Expected patterns:\")\n",
    "print(\"  - 1e-4: Too slow, doesn't reach 200 in 50k steps\")\n",
    "print(\"  - 3e-4: Sweet spot, approaches 200\")\n",
    "print(\"  - 1e-3: Can work but more variable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 Solution (Optional): Longer Training\n",
    "\n",
    "**Expected results after 300k steps:**\n",
    "- Mean reward: 230-270\n",
    "- Much more consistent (lower std)\n",
    "- Clearly solved!\n",
    "\n",
    "**Training curve should show:**\n",
    "- Initial rapid improvement (0-50k steps)\n",
    "- Slower refinement (50k-150k steps)\n",
    "- Plateau/small improvements (150k-300k steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make('LunarLander-v2')\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path='./logs/',\n",
    "    log_path='./logs/',\n",
    "    eval_freq=5000,\n",
    "    n_eval_episodes=20,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "model_long = PPO('MlpPolicy', env, verbose=1, seed=42)\n",
    "\n",
    "print(\"Training for 300k steps...\\n\")\n",
    "model_long.learn(total_timesteps=300_000, callback=eval_callback)\n",
    "\n",
    "mean_long, std_long = evaluate_policy(model_long, env, n_eval_episodes=100)\n",
    "print(f\"\\nFinal: {mean_long:.2f} Â± {std_long:.2f}\")\n",
    "print(f\"\\nâœ“ Expected: 230-270 with low variance\")\n",
    "print(f\"  Longer training â†’ more consistent performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Insights\n",
    "\n",
    "### Why PPO Usually Wins\n",
    "\n",
    "1. **Clipped objective**: Prevents too-large policy updates\n",
    "2. **Multiple epochs**: Reuses data efficiently\n",
    "3. **Robust defaults**: Works well out-of-the-box\n",
    "\n",
    "### When to Use Each Algorithm\n",
    "\n",
    "- **PPO**: Default choice, most reliable\n",
    "- **A2C**: When you need speed and can tolerate variance\n",
    "- **DQN**: Discrete actions, when you have good hyperparameters\n",
    "\n",
    "### Common Issues & Solutions\n",
    "\n",
    "**Problem: Agent doesn't learn (stuck at -200)**\n",
    "- Check learning rate (try 3e-4)\n",
    "- Train longer (100k â†’ 300k steps)\n",
    "- Try different random seed\n",
    "\n",
    "**Problem: High variance in performance**\n",
    "- Increase batch size (64 â†’ 128)\n",
    "- Use more evaluation episodes (20 â†’ 100)\n",
    "- Train longer for stability\n",
    "\n",
    "**Problem: Training is too slow**\n",
    "- Reduce timesteps for experiments (100k â†’ 50k)\n",
    "- Use fewer evaluation episodes\n",
    "- Try A2C (faster but less stable)\n",
    "\n",
    "### Hyperparameter Cheat Sheet\n",
    "\n",
    "**Conservative (stable but slow):**\n",
    "```python\n",
    "PPO(..., learning_rate=1e-4, n_steps=2048, batch_size=128)\n",
    "```\n",
    "\n",
    "**Balanced (recommended):**\n",
    "```python\n",
    "PPO(..., learning_rate=3e-4, n_steps=2048, batch_size=64)\n",
    "```\n",
    "\n",
    "**Aggressive (fast but risky):**\n",
    "```python\n",
    "PPO(..., learning_rate=1e-3, n_steps=1024, batch_size=32)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Expected Results\n",
    "\n",
    "| Task | Algorithm | Steps | Expected Reward |\n",
    "|------|-----------|-------|----------------|\n",
    "| 1 | Random | - | -150 to -250 |\n",
    "| 2 | PPO | 100k | 180 to 240 |\n",
    "| 3 | A2C | 100k | 120 to 200 |\n",
    "| 3 | DQN | 100k | 150 to 220 |\n",
    "| 4 | PPO (LR=1e-4) | 50k | 100 to 160 |\n",
    "| 4 | PPO (LR=3e-4) | 50k | 180 to 220 |\n",
    "| 4 | PPO (LR=1e-3) | 50k | 140 to 200 |\n",
    "| 5 | PPO | 300k | 230 to 270 |\n",
    "\n",
    "**Remember:** These are typical ranges. Your exact results will vary based on:\n",
    "- Random seed\n",
    "- Hardware (CPU vs GPU)\n",
    "- Library versions\n",
    "\n",
    "As long as you're in the ballpark, you're doing fine! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
