{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Deep RL with LunarLander\n",
    "\n",
    "In this lab, you'll use **stable-baselines3**, a popular RL library, to train agents on the LunarLander environment. You'll experiment with different algorithms and hyperparameters to understand what works and why.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ The LunarLander Task\n",
    "\n",
    "**Goal:** Land a spacecraft safely on the landing pad\n",
    "\n",
    "**State (8 values):**\n",
    "- x, y position\n",
    "- x, y velocity\n",
    "- angle, angular velocity\n",
    "- left leg contact, right leg contact\n",
    "\n",
    "**Actions (4 discrete):**\n",
    "- 0: Do nothing\n",
    "- 1: Fire left engine\n",
    "- 2: Fire main engine\n",
    "- 3: Fire right engine\n",
    "\n",
    "**Rewards:**\n",
    "- +100 to +140 for landing safely\n",
    "- -100 for crashing\n",
    "- Small penalties for firing engines (fuel efficiency)\n",
    "- Bonus for legs touching ground\n",
    "\n",
    "**Success:** Average reward â‰¥ 200 over 100 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Der Befehl \"apt-get\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Ã— Building wheel for box2d-py (pyproject.toml) did not run successfully.\n",
      "  â”‚ exit code: 1\n",
      "  â•°â”€> [29 lines of output]\n",
      "      Using setuptools (version 80.9.0).\n",
      "      C:\\Users\\Matias\\AppData\\Local\\Temp\\pip-build-env-lpe2_y7m\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py:289: UserWarning: Unknown distribution option: 'test_suite'\n",
      "        warnings.warn(msg)\n",
      "      C:\\Users\\Matias\\AppData\\Local\\Temp\\pip-build-env-lpe2_y7m\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "      !!\n",
      "      \n",
      "              ********************************************************************************\n",
      "              Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "      \n",
      "              License :: OSI Approved :: zlib/libpng License\n",
      "      \n",
      "              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "              ********************************************************************************\n",
      "      \n",
      "      !!\n",
      "        self._finalize_license_expression()\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\\lib.win-amd64-cpython-312\\Box2D\n",
      "      copying library\\Box2D\\Box2D.py -> build\\lib.win-amd64-cpython-312\\Box2D\n",
      "      copying library\\Box2D\\__init__.py -> build\\lib.win-amd64-cpython-312\\Box2D\n",
      "      creating build\\lib.win-amd64-cpython-312\\Box2D\\b2\n",
      "      copying library\\Box2D\\b2\\__init__.py -> build\\lib.win-amd64-cpython-312\\Box2D\\b2\n",
      "      running build_ext\n",
      "      building 'Box2D._Box2D' extension\n",
      "      swigging Box2D\\Box2D.i to Box2D\\Box2D_wrap.cpp\n",
      "      swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\\Box2D_wrap.cpp Box2D\\Box2D.i\n",
      "      error: command 'swig.exe' failed: None\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for box2d-py\n",
      "error: failed-wheel-build-for-install\n",
      "\n",
      "Ã— Failed to build installable wheels for some pyproject.toml based projects\n",
      "â•°â”€> box2d-py\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y swig\n",
    "!pip install stable-baselines3[extra] gymnasium[box2d] -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO, A2C, DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: Exploring the Environment\n",
    "\n",
    "Let's first understand the environment by watching a random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
    "\n",
    "print(\"Environment Info:\")\n",
    "print(f\"  Observation space: {env.observation_space}\")\n",
    "print(f\"  Action space: {env.action_space}\")\n",
    "print(f\"  Actions: 0=nothing, 1=left, 2=main, 3=right\")\n",
    "\n",
    "# Test random policy\n",
    "state, _ = env.reset(seed=42)\n",
    "print(f\"\\nInitial state: {state}\")\n",
    "print(f\"  [x, y, vx, vy, angle, angular_vel, leg1_contact, leg2_contact]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_random_policy(env, n_episodes=10):\n",
    "    \"\"\"Evaluate a random policy.\"\"\"\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = env.action_space.sample()  # Random action\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "    \n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "# Evaluate random policy\n",
    "print(\"Evaluating random policy...\\n\")\n",
    "random_rewards = evaluate_random_policy(env, n_episodes=10)\n",
    "\n",
    "print(f\"Random Policy Performance:\")\n",
    "print(f\"  Mean reward: {np.mean(random_rewards):.2f}\")\n",
    "print(f\"  Std: {np.std(random_rewards):.2f}\")\n",
    "print(f\"  Min: {np.min(random_rewards):.2f}\")\n",
    "print(f\"  Max: {np.max(random_rewards):.2f}\")\n",
    "print(f\"\\nRandom agents typically crash! (rewards around -200)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Training Your First Agent (PPO)\n",
    "\n",
    "**PPO (Proximal Policy Optimization)** is a popular, reliable algorithm that works well out-of-the-box.\n",
    "\n",
    "### What you'll do:\n",
    "1. Create a PPO agent with default settings\n",
    "2. Train for a fixed number of timesteps\n",
    "3. Evaluate the trained agent\n",
    "\n",
    "**Key hyperparameters to notice:**\n",
    "- `learning_rate`: How fast the agent learns\n",
    "- `n_steps`: How many steps to collect before updating\n",
    "- `batch_size`: Size of minibatches for optimization\n",
    "- `n_epochs`: Number of optimization epochs per update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make('LunarLander-v3')\n",
    "\n",
    "# Create PPO agent with DEFAULT settings\n",
    "model_ppo = PPO(\n",
    "    'MlpPolicy',           # Multi-layer perceptron policy\n",
    "    env,\n",
    "    learning_rate=3e-4,    # Default learning rate\n",
    "    n_steps=2048,          # Number of steps to collect per update\n",
    "    batch_size=64,         # Minibatch size\n",
    "    n_epochs=10,           # Number of epochs per update\n",
    "    gamma=0.99,            # Discount factor\n",
    "    verbose=1,             # Print training progress\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\\nCreated PPO agent with default hyperparameters\")\n",
    "print(\"Ready to train!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "print(\"Training PPO agent...\")\n",
    "print(\"This will take ~5 minutes\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "model_ppo.learn(total_timesteps=100_000)  # 100k steps\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining completed in {train_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model_ppo, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"\\nPPO Agent Performance (100 episodes):\")\n",
    "print(f\"  Mean reward: {mean_reward:.2f} Â± {std_reward:.2f}\")\n",
    "\n",
    "if mean_reward >= 200:\n",
    "    print(f\"  âœ“ SUCCESS! Agent solved LunarLander!\")\n",
    "else:\n",
    "    print(f\"  Agent is learning but needs more training\")\n",
    "    print(f\"  (Success threshold: 200)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Trained Agent\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "def record_video(env_name, model, video_length=500, prefix=''):\n",
    "    \"\"\"\n",
    "    Record a video of the agent playing.\n",
    "    \"\"\"\n",
    "    # Create environment with rgb_array rendering\n",
    "    eval_env = gym.make(env_name, render_mode='rgb_array')\n",
    "    \n",
    "    obs, _ = eval_env.reset()\n",
    "    frames = []\n",
    "    total_reward = 0\n",
    "    \n",
    "    for _ in range(video_length):\n",
    "        frames.append(eval_env.render())\n",
    "        \n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    eval_env.close()\n",
    "    \n",
    "    # Save video\n",
    "    video_path = f\"{prefix}lunar_lander.mp4\"\n",
    "    imageio.mimsave(video_path, frames, fps=30)\n",
    "    \n",
    "    return video_path, total_reward\n",
    "\n",
    "\n",
    "# Record and display video\n",
    "print(\"Recording agent performance...\")\n",
    "video_path, episode_reward = record_video('LunarLander-v3', model_ppo, video_length=1000)\n",
    "\n",
    "print(f\"Episode reward: {episode_reward:.2f}\")\n",
    "print(f\"Result: {'âœ“ LANDED!' if episode_reward > 200 else 'âœ— Crashed'}\")\n",
    "\n",
    "# Display video in Colab\n",
    "from IPython.display import Video\n",
    "Video(video_path, embed=True, width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 3: Comparing Algorithms\n",
    "\n",
    "Now let's compare PPO with other algorithms:\n",
    "- **A2C (Advantage Actor-Critic)**: Simpler, faster, but less stable\n",
    "- **DQN (Deep Q-Network)**: Value-based, good for discrete actions\n",
    "\n",
    "**Your task:** Train A2C and DQN with similar compute budgets and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train A2C\n",
    "print(\"Training A2C agent...\\n\")\n",
    "\n",
    "env = gym.make('LunarLander-v3')\n",
    "model_a2c = A2C(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    learning_rate=7e-4,    # A2C often needs higher LR\n",
    "    n_steps=5,             # A2C uses fewer steps per update\n",
    "    gamma=0.99,\n",
    "    verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "model_a2c.learn(total_timesteps=100_000)\n",
    "mean_a2c, std_a2c = evaluate_policy(model_a2c, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"\\nA2C Performance: {mean_a2c:.2f} Â± {std_a2c:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DQN\n",
    "print(\"Training DQN agent...\\n\")\n",
    "\n",
    "env = gym.make('LunarLander-v3')\n",
    "model_dqn = DQN(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    learning_rate=1e-4,\n",
    "    buffer_size=50000,     # Replay buffer size\n",
    "    learning_starts=1000,  # Start learning after this many steps\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "model_dqn.learn(total_timesteps=100_000)\n",
    "mean_dqn, std_dqn = evaluate_policy(model_dqn, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"\\nDQN Performance: {mean_dqn:.2f} Â± {std_dqn:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all algorithms\n",
    "algorithms = ['Random', 'PPO', 'A2C', 'DQN']\n",
    "mean_rewards = [\n",
    "    np.mean(random_rewards),\n",
    "    mean_reward,\n",
    "    mean_a2c,\n",
    "    mean_dqn\n",
    "]\n",
    "std_rewards = [\n",
    "    np.std(random_rewards),\n",
    "    std_reward,\n",
    "    std_a2c,\n",
    "    std_dqn\n",
    "]\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "bars = plt.bar(algorithms, mean_rewards, yerr=std_rewards, \n",
    "               capsize=5, color=colors, alpha=0.7)\n",
    "plt.axhline(y=200, color='black', linestyle='--', label='Success threshold (200)')\n",
    "plt.ylabel('Mean Reward')\n",
    "plt.title('Algorithm Comparison on LunarLander-v2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, mean in zip(bars, mean_rewards):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{mean:.1f}',\n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(f\"  - PPO typically performs best (most reliable)\")\n",
    "print(f\"  - A2C learns faster but can be less stable\")\n",
    "print(f\"  - DQN is sample-efficient but sensitive to hyperparameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 4: Hyperparameter Experiments\n",
    "\n",
    "Now experiment with hyperparameters to see their effect!\n",
    "\n",
    "**Your task:** Try different learning rates with PPO and observe the impact.\n",
    "\n",
    "**What to adjust:**\n",
    "- `learning_rate`: Higher = faster learning but less stable\n",
    "- Try: `1e-4` (low), `3e-4` (default), `1e-3` (high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(learning_rate, timesteps=50_000):\n",
    "    \"\"\"Train PPO with given learning rate and evaluate.\"\"\"\n",
    "    env = gym.make('LunarLander-v3')\n",
    "    \n",
    "    model = PPO(\n",
    "        'MlpPolicy',\n",
    "        env,\n",
    "        learning_rate=learning_rate,\n",
    "        verbose=0,  # Suppress output for cleaner comparison\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    model.learn(total_timesteps=timesteps)\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=50)\n",
    "    \n",
    "    return mean_reward, std_reward\n",
    "\n",
    "\n",
    "# Experiment with different learning rates\n",
    "learning_rates = [1e-4, 3e-4, 1e-3]\n",
    "results = {}\n",
    "\n",
    "print(\"Experimenting with learning rates...\")\n",
    "print(\"(Training 3 agents with 50k steps each - takes ~5 min)\\n\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"Training with LR={lr}...\")\n",
    "    mean, std = train_and_evaluate(lr, timesteps=50_000)\n",
    "    results[lr] = (mean, std)\n",
    "    print(f\"  Result: {mean:.2f} Â± {std:.2f}\\n\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "lrs = [f\"{lr:.0e}\" for lr in learning_rates]\n",
    "means = [results[lr][0] for lr in learning_rates]\n",
    "stds = [results[lr][1] for lr in learning_rates]\n",
    "\n",
    "plt.bar(lrs, means, yerr=stds, capsize=5, alpha=0.7, color='steelblue')\n",
    "plt.axhline(y=200, color='g', linestyle='--', label='Success threshold')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Mean Reward')\n",
    "plt.title('Impact of Learning Rate on PPO Performance')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWhat did you observe?\")\n",
    "print(\"  - Too low: Slow learning, might not solve in time\")\n",
    "print(\"  - Default: Good balance\")\n",
    "print(\"  - Too high: Might be unstable or overshoot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 5 (Optional): Training Longer\n",
    "\n",
    "The previous experiments used 50k-100k steps for speed. But can we do better with more training?\n",
    "\n",
    "**Your task:** Train PPO for longer (300k steps) and track learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation callback to track progress\n",
    "eval_env = gym.make('LunarLander-v3')\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path='./logs/',\n",
    "    log_path='./logs/',\n",
    "    eval_freq=5000,  # Evaluate every 5k steps\n",
    "    n_eval_episodes=20,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# Train for longer\n",
    "env = gym.make('LunarLander-v3')\n",
    "model_long = PPO('MlpPolicy', env, verbose=1, seed=42)\n",
    "\n",
    "print(\"Training PPO for 300k steps...\")\n",
    "print(\"(This takes ~15 minutes)\\n\")\n",
    "\n",
    "model_long.learn(total_timesteps=300_000, callback=eval_callback)\n",
    "\n",
    "# Final evaluation\n",
    "mean_long, std_long = evaluate_policy(model_long, env, n_eval_episodes=100)\n",
    "print(f\"\\nFinal performance: {mean_long:.2f} Â± {std_long:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **Using RL libraries**: stable-baselines3 makes it easy to try different algorithms\n",
    "2. **Algorithm comparison**: PPO is reliable, A2C is fast, DQN needs tuning\n",
    "3. **Hyperparameter impact**: Learning rate significantly affects performance\n",
    "4. **Training time**: More training usually helps, but with diminishing returns\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**What makes RL challenging:**\n",
    "- Sparse rewards (only get signal at landing/crash)\n",
    "- Delayed consequences (actions affect future states)\n",
    "- High variance in training\n",
    "- Sensitive to hyperparameters\n",
    "\n",
    "**What helps:**\n",
    "- Good algorithm choice (PPO is often best default)\n",
    "- Sufficient training time\n",
    "- Appropriate learning rate\n",
    "- Multiple random seeds for robustness\n",
    "\n",
    "### Things You Can Try\n",
    "\n",
    "1. **Different environments**: Try `LunarLanderContinuous-v3` (continuous actions)\n",
    "2. **Network architecture**: Modify `policy_kwargs` to use bigger networks\n",
    "3. **Reward shaping**: Modify the environment to give better feedback\n",
    "4. **Curriculum learning**: Start with easier tasks, gradually increase difficulty\n",
    "5. **Ensemble methods**: Train multiple agents and combine their predictions\n",
    "\n",
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "You're now ready to work on your project! Choose from:\n",
    "- Atari games (Pong, Breakout)\n",
    "- Autonomous driving (Highway-Env)\n",
    "- Stock trading (Financial RL)\n",
    "- Multi-agent scenarios (PettingZoo)\n",
    "\n",
    "Or propose your own project idea!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
