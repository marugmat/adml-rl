{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Lab Solutions - Q-Learning for Nim\n",
    "\n",
    "---\n",
    "\n",
    "This notebook contains complete solutions with detailed explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nim Environment (Provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NimGame:\n",
    "    \"\"\"Nim game environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_piles):\n",
    "        self.initial_piles = list(initial_piles)\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.piles = list(self.initial_piles)\n",
    "        self.done = False\n",
    "        self.current_player = 0\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        return tuple(self.piles)\n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        actions = []\n",
    "        for pile_idx, pile_size in enumerate(self.piles):\n",
    "            for num_remove in range(1, pile_size + 1):\n",
    "                actions.append((pile_idx, num_remove))\n",
    "        return actions\n",
    "    \n",
    "    def step(self, action):\n",
    "        pile_idx, num_remove = action\n",
    "        self.piles[pile_idx] -= num_remove\n",
    "        \n",
    "        if sum(self.piles) == 0:\n",
    "            self.done = True\n",
    "            return self.get_state(), -1, True\n",
    "        \n",
    "        self.current_player = 1 - self.current_player\n",
    "        return self.get_state(), 0, False\n",
    "    \n",
    "    def render(self):\n",
    "        print(\"\\n\" + \"=\" * 30)\n",
    "        print(f\"Player {self.current_player + 1}'s turn\")\n",
    "        print(\"=\" * 30)\n",
    "        for i, pile in enumerate(self.piles):\n",
    "            objects = \"●\" * pile if pile > 0 else \"(empty)\"\n",
    "            print(f\"Pile {i}: {objects}  ({pile})\")\n",
    "        print()\n",
    "\n",
    "\n",
    "def random_agent(game):\n",
    "    return random.choice(game.get_valid_actions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4 Solution: Setting Hyperparameters\n",
    "\n",
    "### Explanation of Hyperparameters\n",
    "\n",
    "**α (alpha) - Learning Rate = 0.1**\n",
    "- Controls how much new information overrides old information\n",
    "- Too high (e.g., 0.9): Unstable learning, values fluctuate wildly\n",
    "- Too low (e.g., 0.01): Very slow learning\n",
    "- 0.1 is a good default that balances speed and stability\n",
    "\n",
    "**γ (gamma) - Discount Factor = 0.9**\n",
    "- Controls importance of future rewards vs immediate rewards\n",
    "- γ = 1.0: Future rewards are equally important\n",
    "- γ = 0: Only immediate rewards matter\n",
    "- 0.9 gives good weight to future while still preferring sooner rewards\n",
    "\n",
    "**ε (epsilon) - Exploration Rate = 0.3**\n",
    "- Probability of taking a random action\n",
    "- Too low: Agent gets stuck in local optima\n",
    "- Too high: Too much random exploration, slow convergence\n",
    "- 0.3 during training allows good exploration; use 0 during evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5 Solution: Q-Learning Update\n",
    "\n",
    "### The Complete Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Q-Learning agent for Nim - COMPLETE SOLUTION.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.3):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(float)\n",
    "    \n",
    "    def get_q_value(self, state, action):\n",
    "        return self.Q[(state, action)]\n",
    "    \n",
    "    def get_best_action(self, state, valid_actions):\n",
    "        q_values = [self.get_q_value(state, a) for a in valid_actions]\n",
    "        max_q = max(q_values)\n",
    "        best_actions = [a for a, q in zip(valid_actions, q_values) if q == max_q]\n",
    "        return random.choice(best_actions)\n",
    "    \n",
    "    def select_action(self, game, training=True):\n",
    "        state = game.get_state()\n",
    "        valid_actions = game.get_valid_actions()\n",
    "        \n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.choice(valid_actions)\n",
    "        else:\n",
    "            return self.get_best_action(state, valid_actions)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, next_valid_actions, done):\n",
    "        \"\"\"\n",
    "        Q-Learning update rule:\n",
    "        Q(s,a) <- Q(s,a) + alpha * [reward + gamma * max_a' Q(s',a') - Q(s,a)]\n",
    "        \"\"\"\n",
    "        # Step 1: Get current Q-value\n",
    "        current_q = self.get_q_value(state, action)\n",
    "        \n",
    "        # Step 2: Calculate target\n",
    "        if done:\n",
    "            # SOLUTION: Terminal state - just use reward\n",
    "            # No future states, so no discounted future value\n",
    "            target = reward\n",
    "        else:\n",
    "            # SOLUTION: Get max Q-value for next state\n",
    "            next_q_values = [self.get_q_value(next_state, a) for a in next_valid_actions]\n",
    "            max_next_q = max(next_q_values)  # SOLUTION: Take the maximum\n",
    "            \n",
    "            # SOLUTION: Target = reward + discounted future value\n",
    "            target = reward + self.gamma * max_next_q\n",
    "        \n",
    "        # Step 3: Update Q-value\n",
    "        # SOLUTION: Apply the Q-learning update\n",
    "        new_q = current_q + self.alpha * (target - current_q)\n",
    "        \n",
    "        self.Q[(state, action)] = new_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed Explanation of the Update\n",
    "\n",
    "Let's trace through the update step by step:\n",
    "\n",
    "**Example: Non-terminal state**\n",
    "- State s = (3, 4) - piles have 3 and 4 objects\n",
    "- Action a = (0, 1) - remove 1 from pile 0\n",
    "- Reward r = 0 (game continues)\n",
    "- Next state s' = (2, 4)\n",
    "\n",
    "```\n",
    "current_q = Q[(3,4), (0,1)]  # e.g., 0.2\n",
    "max_next_q = max(Q[(2,4), a'] for all valid a')  # e.g., 0.5\n",
    "target = 0 + 0.9 * 0.5 = 0.45\n",
    "new_q = 0.2 + 0.1 * (0.45 - 0.2) = 0.2 + 0.025 = 0.225\n",
    "```\n",
    "\n",
    "**Example: Terminal state (agent loses)**\n",
    "- State s = (1,) - one object left\n",
    "- Action a = (0, 1) - take the last object\n",
    "- Reward r = -1 (agent loses)\n",
    "- Done = True\n",
    "\n",
    "```\n",
    "current_q = Q[(1,), (0,1)]  # e.g., 0\n",
    "target = -1  # No future, just reward\n",
    "new_q = 0 + 0.1 * (-1 - 0) = -0.1\n",
    "```\n",
    "\n",
    "Over many updates, Q-values converge to reflect true action values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the implementation\n",
    "agent = QLearningAgent(alpha=0.1, gamma=0.9, epsilon=0.3)\n",
    "\n",
    "# Test case 1: Terminal state with reward\n",
    "agent.update(\n",
    "    state=(1,),\n",
    "    action=(0, 1),\n",
    "    reward=1.0,\n",
    "    next_state=(0,),\n",
    "    next_valid_actions=[],\n",
    "    done=True\n",
    ")\n",
    "result1 = agent.get_q_value((1,), (0, 1))\n",
    "print(f\"Test 1 - Terminal state:\")\n",
    "print(f\"  Q((1,), (0,1)) = {result1:.3f}\")\n",
    "print(f\"  Expected: 0.100 ✓\" if abs(result1 - 0.1) < 0.001 else f\"  Expected: 0.100 ✗\")\n",
    "\n",
    "# Test case 2: Non-terminal state\n",
    "agent.Q[((1,), (0, 1))] = 0.5  # Set known value\n",
    "agent.update(\n",
    "    state=(2,),\n",
    "    action=(0, 1),\n",
    "    reward=0.0,\n",
    "    next_state=(1,),\n",
    "    next_valid_actions=[(0, 1)],\n",
    "    done=False\n",
    ")\n",
    "result2 = agent.get_q_value((2,), (0, 1))\n",
    "expected2 = 0 + 0.1 * (0 + 0.9 * 0.5 - 0)\n",
    "print(f\"\\nTest 2 - Non-terminal state:\")\n",
    "print(f\"  Q((2,), (0,1)) = {result2:.3f}\")\n",
    "print(f\"  Expected: {expected2:.3f} ✓\" if abs(result2 - expected2) < 0.001 else f\"  Expected: {expected2:.3f} ✗\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Training Loop and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, game, opponent, n_episodes=5000, verbose_every=1000):\n",
    "    \"\"\"Train Q-learning agent by playing against opponent.\"\"\"\n",
    "    wins = 0\n",
    "    win_rates = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = game.reset()\n",
    "        agent_history = []\n",
    "        \n",
    "        while not game.done:\n",
    "            if game.current_player == 0:\n",
    "                action = agent.select_action(game, training=True)\n",
    "                agent_history.append((state, action))\n",
    "            else:\n",
    "                action = opponent(game)\n",
    "            \n",
    "            next_state, reward, done = game.step(action)\n",
    "            \n",
    "            if game.current_player == 1 or done:\n",
    "                if len(agent_history) > 0:\n",
    "                    s, a = agent_history[-1]\n",
    "                    \n",
    "                    if done:\n",
    "                        if game.current_player == 1:\n",
    "                            agent_reward = -1\n",
    "                        else:\n",
    "                            agent_reward = 1\n",
    "                    else:\n",
    "                        agent_reward = 0\n",
    "                    \n",
    "                    next_valid = game.get_valid_actions() if not done else []\n",
    "                    agent.update(s, a, agent_reward, next_state, next_valid, done)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        if game.current_player == 1:\n",
    "            wins += 1\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            win_rates.append(wins / (episode + 1))\n",
    "        \n",
    "        if verbose_every and (episode + 1) % verbose_every == 0:\n",
    "            print(f\"Episode {episode + 1}: Win Rate = {wins/(episode+1)*100:.1f}%\")\n",
    "    \n",
    "    return win_rates\n",
    "\n",
    "\n",
    "def evaluate_agent(agent, game, opponent, n_games=1000):\n",
    "    \"\"\"Evaluate agent without exploration.\"\"\"\n",
    "    wins = 0\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "        state = game.reset()\n",
    "        \n",
    "        while not game.done:\n",
    "            if game.current_player == 0:\n",
    "                action = agent.select_action(game, training=False)\n",
    "            else:\n",
    "                action = opponent(game)\n",
    "            state, reward, done = game.step(action)\n",
    "        \n",
    "        if game.current_player == 1:\n",
    "            wins += 1\n",
    "    \n",
    "    return wins / n_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on 2-pile game\n",
    "print(\"Training on [3, 4] piles...\\n\")\n",
    "agent = QLearningAgent(alpha=0.1, gamma=0.9, epsilon=0.3)\n",
    "game = NimGame([3, 4])\n",
    "\n",
    "win_rates = train_agent(agent, game, random_agent, n_episodes=5000)\n",
    "\n",
    "# Evaluate\n",
    "final_win_rate = evaluate_agent(agent, game, random_agent, n_games=1000)\n",
    "print(f\"\\nFinal win rate: {final_win_rate*100:.1f}%\")\n",
    "print(f\"Q-table size: {len(agent.Q)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.arange(100, 5001, 100), win_rates)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Win Rate')\n",
    "plt.title('Q-Learning Agent Win Rate Over Training')\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', label='Random baseline (50%)', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9 Solution: Multiple Pile Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pile_configs = [\n",
    "    [3],\n",
    "    [3, 4],\n",
    "    [3, 4, 5],\n",
    "    [2, 3, 4, 5],\n",
    "]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for piles in pile_configs:\n",
    "    print(f\"\\nTraining on piles: {piles}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    agent = QLearningAgent(alpha=0.1, gamma=0.9, epsilon=0.3)\n",
    "    game = NimGame(piles)\n",
    "    \n",
    "    win_rates = train_agent(agent, game, random_agent, n_episodes=10000, verbose_every=5000)\n",
    "    final_win_rate = evaluate_agent(agent, game, random_agent, n_games=1000)\n",
    "    \n",
    "    results[tuple(piles)] = {\n",
    "        'agent': agent,\n",
    "        'win_rates': win_rates,\n",
    "        'final_rate': final_win_rate,\n",
    "        'q_size': len(agent.Q)\n",
    "    }\n",
    "    \n",
    "    print(f\"Final win rate: {final_win_rate*100:.1f}%\")\n",
    "    print(f\"Q-table size: {len(agent.Q)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and visualization\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY: Performance by Complexity\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for piles, data in results.items():\n",
    "    print(f\"Piles {str(list(piles)):15} | Win Rate: {data['final_rate']*100:5.1f}% | Q-table: {data['q_size']:6} entries\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "labels = [str(list(p)) for p in results.keys()]\n",
    "win_rates_list = [data['final_rate'] * 100 for data in results.values()]\n",
    "plt.bar(labels, win_rates_list, color='steelblue')\n",
    "plt.axhline(y=50, color='r', linestyle='--', label='Random baseline')\n",
    "plt.xlabel('Pile Configuration')\n",
    "plt.ylabel('Win Rate (%)')\n",
    "plt.title('Agent Performance by Complexity')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "q_sizes = [data['q_size'] for data in results.values()]\n",
    "plt.bar(labels, q_sizes, color='coral')\n",
    "plt.xlabel('Pile Configuration')\n",
    "plt.ylabel('Q-table Size')\n",
    "plt.title('State Space Size by Complexity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10 Solutions: Analysis Questions\n",
    "\n",
    "### 1. Q-table Size Growth\n",
    "\n",
    "**Observation:** The Q-table grows exponentially with the number of piles.\n",
    "\n",
    "**Why:**\n",
    "- For a single pile of size n, there are n possible states\n",
    "- For two piles of sizes n and m, there are n × m states\n",
    "- For k piles, state space is approximately ∏(sizes)\n",
    "- Additionally, each state has multiple actions\n",
    "\n",
    "This exponential growth is called the **curse of dimensionality** and is why tabular RL doesn't scale to large problems.\n",
    "\n",
    "### 2. Win Rate vs Complexity\n",
    "\n",
    "**Observation:** Win rate generally decreases with more piles (though still beats random).\n",
    "\n",
    "**Why:**\n",
    "1. **Larger state space:** With more states, the agent needs more experience to visit each state enough times\n",
    "2. **More strategic depth:** The optimal Nim strategy involves XOR operations across all piles\n",
    "3. **Exploration inefficiency:** With more actions per state, random exploration is less effective\n",
    "\n",
    "### 3. No Exploration (ε = 0)\n",
    "\n",
    "With ε = 0, the agent:\n",
    "- Only takes actions it thinks are best (from the start)\n",
    "- Never explores alternative strategies\n",
    "- Gets stuck in local optima\n",
    "- Many state-action pairs never get visited\n",
    "\n",
    "### 4. Agent Strategy\n",
    "\n",
    "The agent learns to:\n",
    "- Avoid leaving a single object (losing position)\n",
    "- Aim for symmetric positions like (1,1) where it can mirror opponent\n",
    "- The optimal strategy involves XOR (Nim-sum), which the agent approximates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus 1 Solution: Self-Play Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_self_play(initial_piles, n_episodes=10000, verbose_every=2000):\n",
    "    \"\"\"Train two agents against each other.\"\"\"\n",
    "    game = NimGame(initial_piles)\n",
    "    \n",
    "    agent_0 = QLearningAgent(alpha=0.1, gamma=0.9, epsilon=0.3)\n",
    "    agent_1 = QLearningAgent(alpha=0.1, gamma=0.9, epsilon=0.3)\n",
    "    \n",
    "    wins = [0, 0]\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = game.reset()\n",
    "        history = {0: [], 1: []}\n",
    "        \n",
    "        while not game.done:\n",
    "            current = game.current_player\n",
    "            agent = agent_0 if current == 0 else agent_1\n",
    "            \n",
    "            action = agent.select_action(game, training=True)\n",
    "            history[current].append((state, action))\n",
    "            \n",
    "            next_state, reward, done = game.step(action)\n",
    "            state = next_state\n",
    "        \n",
    "        # Determine winner (current_player just lost)\n",
    "        loser = 1 - game.current_player\n",
    "        winner = game.current_player\n",
    "        wins[winner] += 1\n",
    "        \n",
    "        # Update both agents\n",
    "        for player_id, agent in [(0, agent_0), (1, agent_1)]:\n",
    "            player_history = history[player_id]\n",
    "            \n",
    "            for i, (s, a) in enumerate(player_history):\n",
    "                if i == len(player_history) - 1:\n",
    "                    r = 1 if player_id == winner else -1\n",
    "                    agent.update(s, a, r, state, [], True)\n",
    "                else:\n",
    "                    next_s = player_history[i + 1][0]\n",
    "                    game.piles = list(next_s)\n",
    "                    next_valid = game.get_valid_actions()\n",
    "                    agent.update(s, a, 0, next_s, next_valid, False)\n",
    "        \n",
    "        if verbose_every and (episode + 1) % verbose_every == 0:\n",
    "            print(f\"Episode {episode + 1}: Agent 0 wins: {wins[0]}, Agent 1 wins: {wins[1]}\")\n",
    "    \n",
    "    return agent_0, agent_1\n",
    "\n",
    "\n",
    "print(\"Training with self-play...\\n\")\n",
    "agent_sp_0, agent_sp_1 = train_self_play([3, 4], n_episodes=10000)\n",
    "\n",
    "# Evaluate self-play agent against random\n",
    "game = NimGame([3, 4])\n",
    "sp_win_rate = evaluate_agent(agent_sp_0, game, random_agent, n_games=1000)\n",
    "print(f\"\\nSelf-play agent vs Random: {sp_win_rate*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus 2 Solution: Epsilon Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecayingEpsilonAgent(QLearningAgent):\n",
    "    \"\"\"Q-Learning with decaying exploration.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.1, gamma=0.9, epsilon_start=1.0, \n",
    "                 epsilon_end=0.01, epsilon_decay=0.9995):\n",
    "        super().__init__(alpha, gamma, epsilon_start)\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Call after each episode to decay epsilon.\"\"\"\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "\n",
    "def train_with_decay(agent, game, opponent, n_episodes=10000, verbose_every=2000):\n",
    "    \"\"\"Training loop with epsilon decay.\"\"\"\n",
    "    wins = 0\n",
    "    win_rates = []\n",
    "    epsilons = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = game.reset()\n",
    "        agent_history = []\n",
    "        \n",
    "        while not game.done:\n",
    "            if game.current_player == 0:\n",
    "                action = agent.select_action(game, training=True)\n",
    "                agent_history.append((state, action))\n",
    "            else:\n",
    "                action = opponent(game)\n",
    "            \n",
    "            next_state, reward, done = game.step(action)\n",
    "            \n",
    "            if game.current_player == 1 or done:\n",
    "                if len(agent_history) > 0:\n",
    "                    s, a = agent_history[-1]\n",
    "                    if done:\n",
    "                        agent_reward = -1 if game.current_player == 1 else 1\n",
    "                    else:\n",
    "                        agent_reward = 0\n",
    "                    next_valid = game.get_valid_actions() if not done else []\n",
    "                    agent.update(s, a, agent_reward, next_state, next_valid, done)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        if game.current_player == 1:\n",
    "            wins += 1\n",
    "        \n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            win_rates.append(wins / (episode + 1))\n",
    "            epsilons.append(agent.epsilon)\n",
    "        \n",
    "        if verbose_every and (episode + 1) % verbose_every == 0:\n",
    "            print(f\"Episode {episode + 1}: Win Rate = {wins/(episode+1)*100:.1f}%, ε = {agent.epsilon:.4f}\")\n",
    "    \n",
    "    return win_rates, epsilons\n",
    "\n",
    "\n",
    "print(\"Training with epsilon decay...\\n\")\n",
    "decay_agent = DecayingEpsilonAgent()\n",
    "game = NimGame([3, 4])\n",
    "\n",
    "win_rates_decay, epsilons = train_with_decay(decay_agent, game, random_agent, n_episodes=10000)\n",
    "final_decay = evaluate_agent(decay_agent, game, random_agent, n_games=1000)\n",
    "\n",
    "print(f\"\\nWith decay: {final_decay*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot epsilon decay\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(np.arange(100, 10001, 100), epsilons)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.title('Epsilon Decay Over Training')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(np.arange(100, 10001, 100), win_rates_decay, label='With decay')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Win Rate')\n",
    "plt.title('Win Rate with Epsilon Decay')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus 3 Solution: SARSA Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent:\n",
    "    \"\"\"SARSA agent - on-policy TD learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.3):\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(float)\n",
    "    \n",
    "    def get_q_value(self, state, action):\n",
    "        return self.Q[(state, action)]\n",
    "    \n",
    "    def get_best_action(self, state, valid_actions):\n",
    "        q_values = [self.get_q_value(state, a) for a in valid_actions]\n",
    "        max_q = max(q_values)\n",
    "        best_actions = [a for a, q in zip(valid_actions, q_values) if q == max_q]\n",
    "        return random.choice(best_actions)\n",
    "    \n",
    "    def select_action(self, game, training=True):\n",
    "        state = game.get_state()\n",
    "        valid_actions = game.get_valid_actions()\n",
    "        \n",
    "        if training and random.random() < self.epsilon:\n",
    "            return random.choice(valid_actions)\n",
    "        else:\n",
    "            return self.get_best_action(state, valid_actions)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, next_action, done):\n",
    "        \"\"\"\n",
    "        SARSA update: uses actual next action, not max.\n",
    "        Q(s,a) <- Q(s,a) + alpha * [r + gamma * Q(s',a') - Q(s,a)]\n",
    "        \"\"\"\n",
    "        current_q = self.get_q_value(state, action)\n",
    "        \n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            next_q = self.get_q_value(next_state, next_action)\n",
    "            target = reward + self.gamma * next_q\n",
    "        \n",
    "        self.Q[(state, action)] = current_q + self.alpha * (target - current_q)\n",
    "\n",
    "\n",
    "print(\"SARSA vs Q-Learning Comparison\")\n",
    "print(\"=\"*40)\n",
    "print(\"\\nQ-Learning (off-policy): Updates toward max Q(s',a')\")\n",
    "print(\"SARSA (on-policy): Updates toward actual next action Q(s',a')\")\n",
    "print(\"\\nFor deterministic games like Nim, both perform similarly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Q-learning works well for small state spaces** like Nim with few piles\n",
    "\n",
    "2. **State space explosion** limits tabular methods - this motivates Deep Q-Networks\n",
    "\n",
    "3. **Exploration is crucial** - ε=0 leads to poor learning\n",
    "\n",
    "4. **Self-play** can produce stronger agents than training against random\n",
    "\n",
    "5. **Epsilon decay** balances exploration early and exploitation later\n",
    "\n",
    "**Next week:** We'll see how to handle large state spaces using function approximation with neural networks (DQN)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
