{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Lab Assignment - Q-Learning for Nim\n",
    "\n",
    "---\n",
    "\n",
    "## The Game of Nim\n",
    "\n",
    "Nim is a mathematical strategy game:\n",
    "- There are one or more **piles** of objects\n",
    "- Two players take turns\n",
    "- On your turn, you must remove **at least one** object from **exactly one** pile\n",
    "- You can remove as many objects as you want from that pile\n",
    "- **The player who takes the last object LOSES**\n",
    "\n",
    "### Why Nim?\n",
    "- Perfect for tabular Q-learning (discrete, finite states)\n",
    "- Trains in seconds\n",
    "- Has a mathematical optimal strategy (but it's not obvious!)\n",
    "- **Challenge:** Can you beat the trained agent?\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will:\n",
    "- Understand how Q-learning applies to turn-based games\n",
    "- Implement the Q-learning update rule\n",
    "- Train an agent and play against it interactively\n",
    "- Experience the exploration-exploitation tradeoff firsthand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Libraries loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Understanding the Nim Environment\n",
    "\n",
    "The environment is provided for you. Let's explore how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NimGame:\n",
    "    \"\"\"\n",
    "    The game of Nim.\n",
    "    \n",
    "    State: tuple of pile sizes, e.g., (3, 4, 5) means 3 piles with 3, 4, and 5 objects\n",
    "    Action: tuple (pile_index, num_to_remove), e.g., (1, 2) means remove 2 from pile 1\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, initial_piles):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            initial_piles: List or tuple of initial pile sizes, e.g., [3, 4, 5]\n",
    "        \"\"\"\n",
    "        self.initial_piles = tuple(initial_piles)\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Start a new game.\"\"\"\n",
    "        self.piles = list(self.initial_piles)\n",
    "        self.current_player = 1  # Player 1 starts\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"Return current state as a tuple (hashable).\"\"\"\n",
    "        return tuple(self.piles)\n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        \"\"\"\n",
    "        Return list of valid actions.\n",
    "        Each action is (pile_index, num_to_remove).\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        for pile_idx, pile_size in enumerate(self.piles):\n",
    "            for num_remove in range(1, pile_size + 1):\n",
    "                actions.append((pile_idx, num_remove))\n",
    "        return actions\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action.\n",
    "        \n",
    "        Args:\n",
    "            action: (pile_index, num_to_remove)\n",
    "            \n",
    "        Returns:\n",
    "            new_state, reward, done\n",
    "        \"\"\"\n",
    "        pile_idx, num_remove = action\n",
    "        \n",
    "        # Validate action\n",
    "        if pile_idx < 0 or pile_idx >= len(self.piles):\n",
    "            raise ValueError(f\"Invalid pile index: {pile_idx}\")\n",
    "        if num_remove < 1 or num_remove > self.piles[pile_idx]:\n",
    "            raise ValueError(f\"Invalid remove amount: {num_remove} from pile with {self.piles[pile_idx]}\")\n",
    "        \n",
    "        # Make the move\n",
    "        self.piles[pile_idx] -= num_remove\n",
    "        \n",
    "        # Check if game is over (all piles empty)\n",
    "        if sum(self.piles) == 0:\n",
    "            self.done = True\n",
    "            # Current player took the last object, so they LOSE\n",
    "            self.winner = 3 - self.current_player  # Other player wins\n",
    "            reward = -1  # Taking last object is bad!\n",
    "        else:\n",
    "            reward = 0\n",
    "        \n",
    "        # Switch player\n",
    "        self.current_player = 3 - self.current_player\n",
    "        \n",
    "        return self.get_state(), reward, self.done\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Display the current game state.\"\"\"\n",
    "        print(\"\\nPiles:\")\n",
    "        for i, size in enumerate(self.piles):\n",
    "            objects = '|' * size if size > 0 else '(empty)'\n",
    "            print(f\"  Pile {i}: {objects} ({size})\")\n",
    "        print(f\"Player {self.current_player}'s turn\")\n",
    "\n",
    "\n",
    "print(\"NimGame class loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Explore the Environment\n",
    "\n",
    "Run the cells below to understand how the game works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple game with one pile of 5 objects\n",
    "game = NimGame([5])\n",
    "game.render()\n",
    "\n",
    "print(\"\\nValid actions:\", game.get_valid_actions())\n",
    "print(\"State:\", game.get_state())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's play a few moves\n",
    "print(\"Player 1 removes 2 from pile 0:\")\n",
    "state, reward, done = game.step((0, 2))  # Remove 2 from pile 0\n",
    "game.render()\n",
    "print(f\"Reward: {reward}, Done: {done}\")\n",
    "\n",
    "print(\"\\nPlayer 2 removes 2 from pile 0:\")\n",
    "state, reward, done = game.step((0, 2))  # Remove 2 from pile 0\n",
    "game.render()\n",
    "print(f\"Reward: {reward}, Done: {done}\")\n",
    "\n",
    "print(\"\\nPlayer 1 removes 1 from pile 0 (last object!):\")\n",
    "state, reward, done = game.step((0, 1))  # Remove the last one\n",
    "game.render()\n",
    "print(f\"Reward: {reward}, Done: {done}, Winner: Player {game.winner}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Try a Two-Pile Game\n",
    "\n",
    "**Your turn:** Change the pile configuration below to `[3, 4]` (two piles) and run the cell to see how the valid actions change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change [5] to [3, 4] to create a two-pile game\n",
    "game = NimGame([5])\n",
    "game.render()\n",
    "\n",
    "print(\"\\nValid actions:\")\n",
    "for action in game.get_valid_actions():\n",
    "    pile_idx, num_remove = action\n",
    "    print(f\"  Remove {num_remove} from pile {pile_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Random Agent\n",
    "\n",
    "First, let's create a random agent to serve as a baseline and training partner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\"An agent that picks random valid actions.\"\"\"\n",
    "    \n",
    "    def choose_action(self, state, valid_actions):\n",
    "        return random.choice(valid_actions)\n",
    "\n",
    "\n",
    "# Test random agent\n",
    "random_agent = RandomAgent()\n",
    "game = NimGame([3, 4])\n",
    "\n",
    "print(\"Random agent playing:\")\n",
    "state = game.reset()\n",
    "while not game.done:\n",
    "    game.render()\n",
    "    action = random_agent.choose_action(state, game.get_valid_actions())\n",
    "    print(f\"  -> Removes {action[1]} from pile {action[0]}\")\n",
    "    state, reward, done = game.step(action)\n",
    "\n",
    "game.render()\n",
    "print(f\"\\nWinner: Player {game.winner}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Q-Learning Agent\n",
    "\n",
    "Now let's build the Q-Learning agent. Most of the code is provided - you just need to fill in the key parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Q-Learning agent for Nim.\n",
    "    \n",
    "    The Q-table maps (state, action) pairs to expected values.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, epsilon=0.1, alpha=0.5, gamma=0.9):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            epsilon: Exploration rate (probability of random action)\n",
    "            alpha: Learning rate (how fast to update Q-values)\n",
    "            gamma: Discount factor (importance of future rewards)\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Q-table: defaultdict returns 0.0 for unseen (state, action) pairs\n",
    "        self.Q = defaultdict(float)\n",
    "        \n",
    "        # History of moves in current game (for learning)\n",
    "        self.history = []\n",
    "    \n",
    "    def choose_action(self, state, valid_actions):\n",
    "        \"\"\"\n",
    "        Choose an action using epsilon-greedy strategy.\n",
    "        \n",
    "        With probability epsilon: choose randomly (explore)\n",
    "        Otherwise: choose the action with highest Q-value (exploit)\n",
    "        \"\"\"\n",
    "        # Exploration: random action\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(valid_actions)\n",
    "        \n",
    "        # Exploitation: best action based on Q-values\n",
    "        q_values = [self.Q[(state, action)] for action in valid_actions]\n",
    "        max_q = max(q_values)\n",
    "        \n",
    "        # If multiple actions have the same Q-value, pick randomly among them\n",
    "        best_actions = [a for a, q in zip(valid_actions, q_values) if q == max_q]\n",
    "        return random.choice(best_actions)\n",
    "    \n",
    "    def store_transition(self, state, action):\n",
    "        \"\"\"Remember this move for learning later.\"\"\"\n",
    "        self.history.append((state, action))\n",
    "    \n",
    "    def learn(self, final_reward):\n",
    "        \"\"\"\n",
    "        Update Q-values based on the game outcome.\n",
    "        \n",
    "        We work backwards through the history, propagating the reward.\n",
    "        \n",
    "        Args:\n",
    "            final_reward: 1 if won, -1 if lost, 0 for draw\n",
    "        \"\"\"\n",
    "        reward = final_reward\n",
    "        \n",
    "        # Process moves in reverse order (from game end to start)\n",
    "        for state, action in reversed(self.history):\n",
    "            # Current Q-value\n",
    "            current_q = self.Q[(state, action)]\n",
    "            \n",
    "            # ============================================================\n",
    "            # TASK 3.1: Implement the Q-learning update\n",
    "            # ============================================================\n",
    "            # Update formula: Q(s,a) = Q(s,a) + alpha * (target - Q(s,a))\n",
    "            # Where target = reward (we're working backwards, so no future state)\n",
    "            #\n",
    "            # Hint: This is similar to the bandit update from Week 1!\n",
    "            # \n",
    "            # Replace the line below with your implementation:\n",
    "            self.Q[(state, action)] = current_q  # TODO: Update this line\n",
    "            # ============================================================\n",
    "            \n",
    "            # Discount the reward for earlier moves\n",
    "            reward = reward * self.gamma\n",
    "        \n",
    "        # Clear history for next game\n",
    "        self.history = []\n",
    "\n",
    "\n",
    "print(\"QLearningAgent class loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: Complete the Q-Learning Update\n",
    "\n",
    "In the cell above, find the TODO and replace:\n",
    "```python\n",
    "self.Q[(state, action)] = current_q  # TODO: Update this line\n",
    "```\n",
    "\n",
    "With the Q-learning update formula:\n",
    "```python\n",
    "self.Q[(state, action)] = current_q + self.alpha * (reward - current_q)\n",
    "```\n",
    "\n",
    "This moves the Q-value toward the observed reward, at a rate controlled by `alpha`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Training Loop\n",
    "\n",
    "The training loop plays many games between our agent and an opponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, opponent, game, n_games=1000, agent_player=1):\n",
    "    \"\"\"\n",
    "    Train the Q-learning agent by playing games.\n",
    "    \n",
    "    Args:\n",
    "        agent: The Q-learning agent to train\n",
    "        opponent: The opponent agent (e.g., RandomAgent)\n",
    "        game: The Nim game environment\n",
    "        n_games: Number of games to play\n",
    "        agent_player: Which player the agent is (1 or 2)\n",
    "    \n",
    "    Returns:\n",
    "        wins, losses: Number of wins and losses\n",
    "    \"\"\"\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "        state = game.reset()\n",
    "        \n",
    "        while not game.done:\n",
    "            valid_actions = game.get_valid_actions()\n",
    "            \n",
    "            if game.current_player == agent_player:\n",
    "                # Agent's turn\n",
    "                action = agent.choose_action(state, valid_actions)\n",
    "                agent.store_transition(state, action)\n",
    "            else:\n",
    "                # Opponent's turn\n",
    "                action = opponent.choose_action(state, valid_actions)\n",
    "            \n",
    "            state, reward, done = game.step(action)\n",
    "        \n",
    "        # Game over - learn from result\n",
    "        if game.winner == agent_player:\n",
    "            agent.learn(1.0)  # Win!\n",
    "            wins += 1\n",
    "        else:\n",
    "            agent.learn(-1.0)  # Loss\n",
    "            losses += 1\n",
    "    \n",
    "    return wins, losses\n",
    "\n",
    "\n",
    "print(\"Training function loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Train on a Simple Game (1 Pile)\n",
    "\n",
    "Let's start with the simplest case: a single pile of 5 objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent and game\n",
    "agent = QLearningAgent(epsilon=0.3, alpha=0.5, gamma=0.9)\n",
    "opponent = RandomAgent()\n",
    "game = NimGame([5])  # Single pile of 5\n",
    "\n",
    "# Train!\n",
    "print(\"Training on 1 pile of 5 objects...\")\n",
    "wins, losses = train_agent(agent, opponent, game, n_games=5000, agent_player=1)\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Wins: {wins} ({wins/50:.1f}%)\")\n",
    "print(f\"Losses: {losses} ({losses/50:.1f}%)\")\n",
    "print(f\"Q-table entries: {len(agent.Q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Inspect What the Agent Learned\n",
    "\n",
    "Let's see the Q-values for the starting position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_q_values(agent, state, valid_actions):\n",
    "    \"\"\"Display Q-values for all valid actions in a state.\"\"\"\n",
    "    print(f\"\\nState: {state}\")\n",
    "    print(\"Q-values:\")\n",
    "    \n",
    "    q_values = [(action, agent.Q[(state, action)]) for action in valid_actions]\n",
    "    q_values.sort(key=lambda x: x[1], reverse=True)  # Sort by Q-value\n",
    "    \n",
    "    for action, q in q_values:\n",
    "        pile_idx, num_remove = action\n",
    "        marker = \" <-- BEST\" if q == max(v[1] for v in q_values) else \"\"\n",
    "        print(f\"  Remove {num_remove} from pile {pile_idx}: Q = {q:+.3f}{marker}\")\n",
    "\n",
    "\n",
    "# Show Q-values for starting state\n",
    "game = NimGame([5])\n",
    "state = game.get_state()\n",
    "show_q_values(agent, state, game.get_valid_actions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What's the Best Opening Move?\n",
    "\n",
    "Look at the Q-values above. Which move does the agent prefer and why?\n",
    "\n",
    "**Hint:** In Nim with one pile, the winning strategy is to leave your opponent with 1 object (so they must take it and lose)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Play Against the Agent!\n",
    "\n",
    "Now let's see if you can beat the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_against_agent(agent, initial_piles, human_player=1):\n",
    "    \"\"\"\n",
    "    Play interactively against the trained agent.\n",
    "    \n",
    "    Args:\n",
    "        agent: The trained Q-learning agent\n",
    "        initial_piles: Starting pile configuration\n",
    "        human_player: 1 to go first, 2 to go second\n",
    "    \"\"\"\n",
    "    game = NimGame(initial_piles)\n",
    "    agent.epsilon = 0  # No exploration during play\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"NIM vs Q-Learning Agent\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"You are Player {human_player} ({'first' if human_player == 1 else 'second'})\")\n",
    "    print(\"Remember: Taking the LAST object means you LOSE!\")\n",
    "    print(\"\\nHow to play:\")\n",
    "    print(\"  Enter: pile_number, amount_to_remove\")\n",
    "    print(\"  Example: 0, 3 (removes 3 from pile 0)\")\n",
    "    \n",
    "    state = game.reset()\n",
    "    \n",
    "    while not game.done:\n",
    "        game.render()\n",
    "        valid_actions = game.get_valid_actions()\n",
    "        \n",
    "        if game.current_player == human_player:\n",
    "            # Human's turn\n",
    "            while True:\n",
    "                try:\n",
    "                    user_input = input(\"Your move (pile, amount): \")\n",
    "                    pile_idx, num_remove = map(int, user_input.replace(' ', '').split(','))\n",
    "                    action = (pile_idx, num_remove)\n",
    "                    if action in valid_actions:\n",
    "                        break\n",
    "                    print(f\"Invalid move! Valid actions: {valid_actions}\")\n",
    "                except (ValueError, IndexError):\n",
    "                    print(\"Please enter: pile_number, amount (e.g., 0, 2)\")\n",
    "        else:\n",
    "            # Agent's turn\n",
    "            action = agent.choose_action(state, valid_actions)\n",
    "            print(f\"Agent removes {action[1]} from pile {action[0]}\")\n",
    "        \n",
    "        state, _, _ = game.step(action)\n",
    "    \n",
    "    # Game over\n",
    "    game.render()\n",
    "    if game.winner == human_player:\n",
    "        print(\"\\nðŸŽ‰ Congratulations! You won!\")\n",
    "    else:\n",
    "        print(\"\\nðŸ¤– The agent wins!\")\n",
    "\n",
    "\n",
    "print(\"Play function loaded!\")\n",
    "print(\"\\nTo play, run: play_against_agent(agent, [5], human_player=1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to play against the agent!\n",
    "# play_against_agent(agent, [5], human_player=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1: Can You Beat the Agent?\n",
    "\n",
    "Try playing as both Player 1 (first) and Player 2 (second). \n",
    "\n",
    "- When you go first with pile [5], what's the winning strategy?\n",
    "- When you go second, can you ever win against a perfect opponent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Increase the Difficulty!\n",
    "\n",
    "Now let's train on more complex configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6.1: Two Piles\n",
    "\n",
    "**Your turn:** Train a new agent on two piles `[3, 4]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a new agent and train on [3, 4]\n",
    "# Hint: Copy and modify the code from Task 4.1\n",
    "\n",
    "agent_2pile = QLearningAgent(epsilon=0.3, alpha=0.5, gamma=0.9)\n",
    "opponent = RandomAgent()\n",
    "game_2pile = NimGame([3, 4])  # Two piles\n",
    "\n",
    "print(\"Training on 2 piles [3, 4]...\")\n",
    "wins, losses = train_agent(agent_2pile, opponent, game_2pile, n_games=10000, agent_player=1)\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Wins: {wins} ({wins/100:.1f}%)\")\n",
    "print(f\"Losses: {losses} ({losses/100:.1f}%)\")\n",
    "print(f\"Q-table entries: {len(agent_2pile.Q)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to beat it!\n",
    "# play_against_agent(agent_2pile, [3, 4], human_player=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6.2: Three Piles\n",
    "\n",
    "Train on `[2, 3, 4]` - this is where it gets interesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on 3 piles\n",
    "agent_3pile = QLearningAgent(epsilon=0.3, alpha=0.5, gamma=0.9)\n",
    "opponent = RandomAgent()\n",
    "game_3pile = NimGame([2, 3, 4])\n",
    "\n",
    "print(\"Training on 3 piles [2, 3, 4]...\")\n",
    "wins, losses = train_agent(agent_3pile, opponent, game_3pile, n_games=20000, agent_player=1)\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Wins: {wins} ({wins/200:.1f}%)\")\n",
    "print(f\"Losses: {losses} ({losses/200:.1f}%)\")\n",
    "print(f\"Q-table entries: {len(agent_3pile.Q)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you beat it now?\n",
    "# play_against_agent(agent_3pile, [2, 3, 4], human_player=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6.3: Four Piles (The Real Challenge!)\n",
    "\n",
    "Train on `[1, 3, 5, 7]` - a classic Nim configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on 4 piles\n",
    "agent_4pile = QLearningAgent(epsilon=0.3, alpha=0.5, gamma=0.9)\n",
    "opponent = RandomAgent()\n",
    "game_4pile = NimGame([1, 3, 5, 7])\n",
    "\n",
    "print(\"Training on 4 piles [1, 3, 5, 7]...\")\n",
    "wins, losses = train_agent(agent_4pile, opponent, game_4pile, n_games=50000, agent_player=1)\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"Wins: {wins} ({wins/500:.1f}%)\")\n",
    "print(f\"Losses: {losses} ({losses/500:.1f}%)\")\n",
    "print(f\"Q-table entries: {len(agent_4pile.Q)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ultimate challenge - can you beat a trained agent on 4 piles?\n",
    "# play_against_agent(agent_4pile, [1, 3, 5, 7], human_player=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: Evaluation\n",
    "\n",
    "Let's evaluate how well our agents perform against random opponents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, game, n_games=1000):\n",
    "    \"\"\"Evaluate agent win rate (no exploration).\"\"\"\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0\n",
    "    \n",
    "    wins_as_p1 = 0\n",
    "    wins_as_p2 = 0\n",
    "    opponent = RandomAgent()\n",
    "    \n",
    "    # Test as Player 1\n",
    "    for _ in range(n_games // 2):\n",
    "        state = game.reset()\n",
    "        while not game.done:\n",
    "            valid_actions = game.get_valid_actions()\n",
    "            if game.current_player == 1:\n",
    "                action = agent.choose_action(state, valid_actions)\n",
    "            else:\n",
    "                action = opponent.choose_action(state, valid_actions)\n",
    "            state, _, _ = game.step(action)\n",
    "        if game.winner == 1:\n",
    "            wins_as_p1 += 1\n",
    "    \n",
    "    # Test as Player 2\n",
    "    for _ in range(n_games // 2):\n",
    "        state = game.reset()\n",
    "        while not game.done:\n",
    "            valid_actions = game.get_valid_actions()\n",
    "            if game.current_player == 2:\n",
    "                action = agent.choose_action(state, valid_actions)\n",
    "            else:\n",
    "                action = opponent.choose_action(state, valid_actions)\n",
    "            state, _, _ = game.step(action)\n",
    "        if game.winner == 2:\n",
    "            wins_as_p2 += 1\n",
    "    \n",
    "    agent.epsilon = original_epsilon\n",
    "    return wins_as_p1 / (n_games // 2), wins_as_p2 / (n_games // 2)\n",
    "\n",
    "\n",
    "# Evaluate all agents\n",
    "print(\"Agent Performance vs Random Opponent:\\n\")\n",
    "\n",
    "configs = [\n",
    "    (\"1 pile [5]\", agent, NimGame([5])),\n",
    "    (\"2 piles [3,4]\", agent_2pile, NimGame([3, 4])),\n",
    "    (\"3 piles [2,3,4]\", agent_3pile, NimGame([2, 3, 4])),\n",
    "    (\"4 piles [1,3,5,7]\", agent_4pile, NimGame([1, 3, 5, 7])),\n",
    "]\n",
    "\n",
    "for name, ag, gm in configs:\n",
    "    win_p1, win_p2 = evaluate_agent(ag, gm, n_games=1000)\n",
    "    print(f\"{name:20s} | As P1: {win_p1*100:5.1f}% | As P2: {win_p2*100:5.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: Analysis Questions\n",
    "\n",
    "Answer these questions based on your experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "Look at your evaluation results. Why does the agent perform better as certain player positions in different configurations?\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "For the 4-pile game [1,3,5,7], what happens if both players play perfectly? \n",
    "\n",
    "(Hint: This is a famous configuration. The XOR of all pile sizes = 1^3^5^7 = 0)\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "Why does the Q-table size grow significantly with more piles?\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## (Optional) Bonus Challenges\n",
    "\n",
    "If you finish early, try these more advanced tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 1: Self-Play Training\n",
    "\n",
    "Train an agent by playing against itself instead of a random opponent. Does it learn better strategies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Implement self-play training\n",
    "# Hint: The agent plays both sides, keeping separate histories\n",
    "# for each player, then learns from both perspectives\n",
    "\n",
    "def train_self_play(agent, game, n_games=10000):\n",
    "    \"\"\"\n",
    "    Train agent by playing against itself.\n",
    "    \n",
    "    TODO: Implement this function\n",
    "    \"\"\"\n",
    "    pass  # Your implementation here\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "# self_play_agent = QLearningAgent(epsilon=0.3, alpha=0.5, gamma=0.9)\n",
    "# train_self_play(self_play_agent, NimGame([3, 4]), n_games=20000)\n",
    "# evaluate_agent(self_play_agent, NimGame([3, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 2: Learning Curve Visualization\n",
    "\n",
    "Track and plot the agent's win rate during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Track learning progress over time\n",
    "# Hint: Evaluate every N training games and store win rates\n",
    "\n",
    "def train_and_track(game, n_games=50000, eval_every=2000):\n",
    "    \"\"\"\n",
    "    Train agent and track win rate over time.\n",
    "    \n",
    "    Returns:\n",
    "        game_numbers: List of training game counts\n",
    "        win_rates: List of win rates at each checkpoint\n",
    "    \"\"\"\n",
    "    # TODO: Implement this function\n",
    "    pass\n",
    "\n",
    "\n",
    "# Plot the learning curve\n",
    "# game_nums, win_rates = train_and_track(NimGame([2, 3, 4]))\n",
    "# plt.plot(game_nums, win_rates)\n",
    "# plt.xlabel('Training Games')\n",
    "# plt.ylabel('Win Rate')\n",
    "# plt.title('Learning Curve')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus 3: Explore Hyperparameters\n",
    "\n",
    "How do different values of epsilon, alpha, and gamma affect learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Compare different hyperparameter settings\n",
    "# Try varying:\n",
    "#   - epsilon: [0.1, 0.2, 0.3, 0.5]\n",
    "#   - alpha: [0.1, 0.3, 0.5, 0.9]\n",
    "#   - gamma: [0.5, 0.9, 0.99]\n",
    "\n",
    "# Your experiments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this lab, you:\n",
    "\n",
    "1. âœ… Learned how Q-learning applies to turn-based games\n",
    "2. âœ… Implemented the Q-learning update rule\n",
    "3. âœ… Trained agents on progressively harder Nim configurations\n",
    "4. âœ… Played interactively against your trained agents\n",
    "5. âœ… Analyzed how complexity affects learning\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Q-learning can discover winning strategies through trial and error\n",
    "- State space size grows exponentially with problem complexity\n",
    "- More training is needed for larger problems\n",
    "- Self-play can lead to stronger strategies than playing vs random\n",
    "\n",
    "**Next Week:** Deep Q-Networks (DQN) - using neural networks when the state space is too large for a table!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
