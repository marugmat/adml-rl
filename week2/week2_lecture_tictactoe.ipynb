{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Q-Learning - Tic-Tac-Toe\n",
    "## Lecture Demonstration\n",
    "\n",
    "**Course:** Reinforcement Learning - Continuing Education  \n",
    "**Institution:** Zurich University of Applied Sciences\n",
    "\n",
    "---\n",
    "\n",
    "## Today's Topics\n",
    "1. From Bandits to Sequential Decision Making\n",
    "2. Value Functions and Q-Values\n",
    "3. The Q-Learning Algorithm\n",
    "4. Value-Based vs Policy-Based Methods\n",
    "5. Live Demo: Training an agent to play Tic-Tac-Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Bandits to MDPs\n",
    "\n",
    "**Last week:** Multi-Armed Bandits\n",
    "- Single state, multiple actions\n",
    "- Immediate rewards only\n",
    "- Goal: Find the best arm\n",
    "\n",
    "**This week:** Markov Decision Processes (MDPs)\n",
    "- Multiple states\n",
    "- Actions cause state transitions\n",
    "- Delayed rewards (actions now affect future)\n",
    "- Goal: Find the best action **in each state**\n",
    "\n",
    "### The Key Insight\n",
    "In an MDP, we need to learn:\n",
    "$$Q(s, a) = \\text{Expected total reward from taking action } a \\text{ in state } s$$\n",
    "\n",
    "This is called the **Q-value** or **action-value function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Value-Based vs Policy-Based Methods\n",
    "\n",
    "### Value-Based (Today's Focus)\n",
    "- Learn the **value** of each state-action pair: $Q(s, a)$\n",
    "- Derive policy by picking action with highest value: $\\pi(s) = \\arg\\max_a Q(s, a)$\n",
    "- Examples: Q-Learning, DQN, SARSA\n",
    "\n",
    "### Policy-Based (Next Weeks)\n",
    "- Learn the **policy** directly: $\\pi(a|s) = P(\\text{action } a | \\text{state } s)$\n",
    "- No need for value function\n",
    "- Examples: REINFORCE, PPO, A2C\n",
    "\n",
    "### Key Difference\n",
    "| Aspect | Value-Based | Policy-Based |\n",
    "|--------|-------------|-------------|\n",
    "| Learns | Q(s,a) values | π(a|s) probabilities |\n",
    "| Policy | Implicit (argmax) | Explicit |\n",
    "| Action space | Discrete (typically) | Discrete or Continuous |\n",
    "| Exploration | ε-greedy, UCB | Built into stochastic policy |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Q-Learning Algorithm\n",
    "\n",
    "### Core Update Rule\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ = learning rate (how fast we update)\n",
    "- $\\gamma$ = discount factor (how much we value future rewards)\n",
    "- $r$ = immediate reward\n",
    "- $s'$ = next state\n",
    "- $\\max_{a'} Q(s', a')$ = value of best action in next state\n",
    "\n",
    "### Intuition\n",
    "- **Target:** $r + \\gamma \\max_{a'} Q(s', a')$ (what we think Q should be)\n",
    "- **Error:** Target - Current estimate\n",
    "- **Update:** Move current estimate toward target\n",
    "\n",
    "### Key Property: Off-Policy\n",
    "Q-learning updates toward the **best** next action, even if we didn't take it. This is called **off-policy** learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tic-Tac-Toe Environment\n",
    "\n",
    "Let's build a complete Tic-Tac-Toe environment to demonstrate Q-learning.\n",
    "\n",
    "**State:** The board configuration (9 positions, each X, O, or empty)  \n",
    "**Actions:** Place your mark in an empty position (0-8)  \n",
    "**Reward:** +1 for win, -1 for loss, 0 for draw or ongoing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    \"\"\"Tic-Tac-Toe game environment.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the board to empty.\"\"\"\n",
    "        self.board = [' '] * 9  # 9 empty positions\n",
    "        self.current_player = 'X'  # X always goes first\n",
    "        self.done = False\n",
    "        self.winner = None\n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"Return board as a tuple (hashable for Q-table).\"\"\"\n",
    "        return tuple(self.board)\n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        \"\"\"Return list of empty positions.\"\"\"\n",
    "        return [i for i in range(9) if self.board[i] == ' ']\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Make a move and return (next_state, reward, done).\"\"\"\n",
    "        if self.board[action] != ' ' or self.done:\n",
    "            raise ValueError(f\"Invalid action: {action}\")\n",
    "        \n",
    "        # Make the move\n",
    "        self.board[action] = self.current_player\n",
    "        \n",
    "        # Check for winner\n",
    "        if self._check_winner(self.current_player):\n",
    "            self.done = True\n",
    "            self.winner = self.current_player\n",
    "            reward = 1 if self.current_player == 'X' else -1\n",
    "            return self.get_state(), reward, True\n",
    "        \n",
    "        # Check for draw\n",
    "        if ' ' not in self.board:\n",
    "            self.done = True\n",
    "            return self.get_state(), 0, True\n",
    "        \n",
    "        # Switch player\n",
    "        self.current_player = 'O' if self.current_player == 'X' else 'X'\n",
    "        return self.get_state(), 0, False\n",
    "    \n",
    "    def _check_winner(self, player):\n",
    "        \"\"\"Check if player has won.\"\"\"\n",
    "        wins = [\n",
    "            [0, 1, 2], [3, 4, 5], [6, 7, 8],  # Rows\n",
    "            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # Columns\n",
    "            [0, 4, 8], [2, 4, 6]              # Diagonals\n",
    "        ]\n",
    "        return any(all(self.board[i] == player for i in combo) for combo in wins)\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Display the board.\"\"\"\n",
    "        print(\"\\n\")\n",
    "        for i in range(3):\n",
    "            row = self.board[i*3:(i+1)*3]\n",
    "            print(f\" {row[0]} | {row[1]} | {row[2]} \")\n",
    "            if i < 2:\n",
    "                print(\"-----------\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Test the environment\n",
    "env = TicTacToe()\n",
    "env.render()\n",
    "print(\"Valid actions:\", env.get_valid_actions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Approach 1: Random Agent (Baseline)\n",
    "\n",
    "Before Q-learning, let's see how a random agent performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent(env):\n",
    "    \"\"\"Select a random valid action.\"\"\"\n",
    "    return random.choice(env.get_valid_actions())\n",
    "\n",
    "def play_game(agent_x, agent_o, env, verbose=False):\n",
    "    \"\"\"Play a game between two agents. Returns winner ('X', 'O', or None for draw).\"\"\"\n",
    "    state = env.reset()\n",
    "    \n",
    "    while not env.done:\n",
    "        if env.current_player == 'X':\n",
    "            action = agent_x(env)\n",
    "        else:\n",
    "            action = agent_o(env)\n",
    "        \n",
    "        state, reward, done = env.step(action)\n",
    "        \n",
    "        if verbose:\n",
    "            env.render()\n",
    "    \n",
    "    return env.winner\n",
    "\n",
    "# Play 1000 games: Random vs Random\n",
    "env = TicTacToe()\n",
    "results = {'X': 0, 'O': 0, 'Draw': 0}\n",
    "\n",
    "for _ in range(1000):\n",
    "    winner = play_game(random_agent, random_agent, env)\n",
    "    if winner == 'X':\n",
    "        results['X'] += 1\n",
    "    elif winner == 'O':\n",
    "        results['O'] += 1\n",
    "    else:\n",
    "        results['Draw'] += 1\n",
    "\n",
    "print(\"Random vs Random (1000 games):\")\n",
    "print(f\"  X wins:  {results['X']} ({results['X']/10:.1f}%)\")\n",
    "print(f\"  O wins:  {results['O']} ({results['O']/10:.1f}%)\")\n",
    "print(f\"  Draws:   {results['Draw']} ({results['Draw']/10:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** X wins more because X goes first. Random play leads to many wins/losses, few draws.\n",
    "\n",
    "Optimal play in Tic-Tac-Toe always results in a draw!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Approach 2: Q-Learning Agent\n",
    "\n",
    "Now let's implement a Q-learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Q-Learning agent for Tic-Tac-Toe.\"\"\"\n",
    "    \n",
    "    def __init__(self, player, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            player: 'X' or 'O'\n",
    "            alpha: Learning rate\n",
    "            gamma: Discount factor\n",
    "            epsilon: Exploration rate\n",
    "        \"\"\"\n",
    "        self.player = player\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Q-table: maps (state, action) -> value\n",
    "        # Using defaultdict so unseen states default to 0\n",
    "        self.Q = defaultdict(float)\n",
    "    \n",
    "    def get_q_value(self, state, action):\n",
    "        \"\"\"Get Q-value for state-action pair.\"\"\"\n",
    "        return self.Q[(state, action)]\n",
    "    \n",
    "    def get_best_action(self, state, valid_actions):\n",
    "        \"\"\"Return action with highest Q-value.\"\"\"\n",
    "        q_values = [self.get_q_value(state, a) for a in valid_actions]\n",
    "        max_q = max(q_values)\n",
    "        # If multiple actions have max Q, choose randomly among them\n",
    "        best_actions = [a for a, q in zip(valid_actions, q_values) if q == max_q]\n",
    "        return random.choice(best_actions)\n",
    "    \n",
    "    def select_action(self, env, training=True):\n",
    "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
    "        state = env.get_state()\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        \n",
    "        if training and random.random() < self.epsilon:\n",
    "            # Explore: random action\n",
    "            return random.choice(valid_actions)\n",
    "        else:\n",
    "            # Exploit: best action\n",
    "            return self.get_best_action(state, valid_actions)\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, next_valid_actions, done):\n",
    "        \"\"\"Update Q-value using Q-learning update rule.\"\"\"\n",
    "        current_q = self.get_q_value(state, action)\n",
    "        \n",
    "        if done:\n",
    "            # Terminal state: no future rewards\n",
    "            target = reward\n",
    "        else:\n",
    "            # Non-terminal: include discounted future value\n",
    "            next_q_values = [self.get_q_value(next_state, a) for a in next_valid_actions]\n",
    "            max_next_q = max(next_q_values) if next_q_values else 0\n",
    "            target = reward + self.gamma * max_next_q\n",
    "        \n",
    "        # Q-learning update\n",
    "        self.Q[(state, action)] = current_q + self.alpha * (target - current_q)\n",
    "\n",
    "print(\"Q-Learning Agent class created!\")\n",
    "print(\"Key components:\")\n",
    "print(\"  - Q-table: stores value of each (state, action) pair\")\n",
    "print(\"  - select_action: epsilon-greedy exploration\")\n",
    "print(\"  - update: Q-learning update rule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training the Q-Learning Agent\n",
    "\n",
    "We'll train the agent by playing many games against itself (self-play)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, opponent_agent, n_episodes=10000, verbose_every=2000):\n",
    "    \"\"\"Train agent by playing against opponent.\"\"\"\n",
    "    env = TicTacToe()\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "    \n",
    "    win_history = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Store trajectory for learning\n",
    "        trajectory = []  # [(state, action, reward), ...]\n",
    "        \n",
    "        while not env.done:\n",
    "            if env.current_player == agent.player:\n",
    "                # Agent's turn\n",
    "                action = agent.select_action(env, training=True)\n",
    "                old_state = state\n",
    "                state, reward, done = env.step(action)\n",
    "                trajectory.append((old_state, action, reward, state, done))\n",
    "            else:\n",
    "                # Opponent's turn\n",
    "                action = opponent_agent(env)\n",
    "                state, reward, done = env.step(action)\n",
    "                \n",
    "                # If opponent just moved and game ended, we need to update\n",
    "                # our last action with the negative reward (we lost or drew)\n",
    "                if done and len(trajectory) > 0:\n",
    "                    s, a, r, ns, d = trajectory[-1]\n",
    "                    trajectory[-1] = (s, a, -reward, state, True)  # Negate opponent's reward\n",
    "        \n",
    "        # Learn from trajectory\n",
    "        for i, (s, a, r, ns, d) in enumerate(trajectory):\n",
    "            next_valid = env.get_valid_actions() if not d else []\n",
    "            agent.update(s, a, r, ns, next_valid, d)\n",
    "        \n",
    "        # Track results\n",
    "        if env.winner == agent.player:\n",
    "            wins += 1\n",
    "        elif env.winner is None:\n",
    "            draws += 1\n",
    "        else:\n",
    "            losses += 1\n",
    "        \n",
    "        # Track win rate over time\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            win_history.append(wins / (episode + 1))\n",
    "        \n",
    "        if verbose_every and (episode + 1) % verbose_every == 0:\n",
    "            print(f\"Episode {episode + 1}: Wins={wins}, Losses={losses}, Draws={draws}, \"\n",
    "                  f\"Win Rate={wins/(episode+1)*100:.1f}%\")\n",
    "    \n",
    "    return win_history\n",
    "\n",
    "# Create and train agent\n",
    "agent_x = QLearningAgent(player='X', alpha=0.1, gamma=0.9, epsilon=0.2)\n",
    "\n",
    "print(\"Training Q-Learning agent (X) against Random opponent...\\n\")\n",
    "win_history = train_agent(agent_x, random_agent, n_episodes=10000)\n",
    "\n",
    "print(f\"\\nQ-table size: {len(agent_x.Q)} state-action pairs learned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.arange(100, 10001, 100), win_history)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Win Rate')\n",
    "plt.title('Q-Learning Agent Win Rate Over Training')\n",
    "plt.axhline(y=0.58, color='r', linestyle='--', label='Random baseline (~58%)', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluating the Trained Agent\n",
    "\n",
    "Let's see how well our trained agent performs against random opponents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, opponent, n_games=1000):\n",
    "    \"\"\"Evaluate agent (no exploration during evaluation).\"\"\"\n",
    "    env = TicTacToe()\n",
    "    results = {'wins': 0, 'losses': 0, 'draws': 0}\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "        state = env.reset()\n",
    "        \n",
    "        while not env.done:\n",
    "            if env.current_player == agent.player:\n",
    "                action = agent.select_action(env, training=False)  # No exploration!\n",
    "            else:\n",
    "                action = opponent(env)\n",
    "            state, reward, done = env.step(action)\n",
    "        \n",
    "        if env.winner == agent.player:\n",
    "            results['wins'] += 1\n",
    "        elif env.winner is None:\n",
    "            results['draws'] += 1\n",
    "        else:\n",
    "            results['losses'] += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate\n",
    "results = evaluate_agent(agent_x, random_agent, n_games=1000)\n",
    "\n",
    "print(\"Trained Q-Learning Agent vs Random (1000 games):\")\n",
    "print(f\"  Wins:   {results['wins']} ({results['wins']/10:.1f}%)\")\n",
    "print(f\"  Losses: {results['losses']} ({results['losses']/10:.1f}%)\")\n",
    "print(f\"  Draws:  {results['draws']} ({results['draws']/10:.1f}%)\")\n",
    "print(f\"\\nCompare to Random vs Random: ~58% wins, ~29% losses, ~13% draws\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** The Q-learning agent wins much more often than the random baseline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Watching the Agent Play\n",
    "\n",
    "Let's watch the trained agent play a game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_game(agent, opponent, agent_is_x=True):\n",
    "    \"\"\"Watch a game between agent and opponent with visualization.\"\"\"\n",
    "    env = TicTacToe()\n",
    "    state = env.reset()\n",
    "    \n",
    "    print(\"=\" * 30)\n",
    "    print(\"Game Start!\")\n",
    "    print(f\"Agent plays: {'X' if agent_is_x else 'O'}\")\n",
    "    print(\"=\" * 30)\n",
    "    env.render()\n",
    "    \n",
    "    move_num = 1\n",
    "    while not env.done:\n",
    "        if (env.current_player == 'X') == agent_is_x:\n",
    "            action = agent.select_action(env, training=False)\n",
    "            player_type = \"Agent\"\n",
    "        else:\n",
    "            action = opponent(env)\n",
    "            player_type = \"Random\"\n",
    "        \n",
    "        print(f\"Move {move_num}: {player_type} ({env.current_player}) plays position {action}\")\n",
    "        state, reward, done = env.step(action)\n",
    "        env.render()\n",
    "        move_num += 1\n",
    "    \n",
    "    print(\"=\" * 30)\n",
    "    if env.winner:\n",
    "        print(f\"Winner: {env.winner}!\")\n",
    "    else:\n",
    "        print(\"It's a draw!\")\n",
    "    print(\"=\" * 30)\n",
    "\n",
    "# Watch a game\n",
    "watch_game(agent_x, random_agent, agent_is_x=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Inspecting Q-Values\n",
    "\n",
    "Let's look at what the agent learned by examining some Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_q_values(agent, env):\n",
    "    \"\"\"Display Q-values for current board state.\"\"\"\n",
    "    state = env.get_state()\n",
    "    valid_actions = env.get_valid_actions()\n",
    "    \n",
    "    print(\"Current board:\")\n",
    "    env.render()\n",
    "    \n",
    "    print(\"Q-values for each position:\")\n",
    "    print(\"(Position: Q-value)\\n\")\n",
    "    \n",
    "    q_display = []\n",
    "    for i in range(9):\n",
    "        if i in valid_actions:\n",
    "            q_val = agent.get_q_value(state, i)\n",
    "            q_display.append(f\"{q_val:+.2f}\")\n",
    "        else:\n",
    "            q_display.append(\"  \" + env.board[i] + \"  \")\n",
    "    \n",
    "    for i in range(3):\n",
    "        row = q_display[i*3:(i+1)*3]\n",
    "        print(f\" {row[0]} | {row[1]} | {row[2]} \")\n",
    "        if i < 2:\n",
    "            print(\"-------------------\")\n",
    "    \n",
    "    best_action = agent.get_best_action(state, valid_actions)\n",
    "    print(f\"\\nBest action: position {best_action}\")\n",
    "\n",
    "# Start a fresh game and show Q-values\n",
    "env = TicTacToe()\n",
    "env.reset()\n",
    "\n",
    "print(\"Empty board - Q-values:\")\n",
    "show_q_values(agent_x, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After some moves\n",
    "env.reset()\n",
    "env.step(4)  # X plays center\n",
    "env.step(0)  # O plays corner\n",
    "\n",
    "print(\"After X plays center, O plays corner:\")\n",
    "show_q_values(agent_x, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A critical position - agent should block!\n",
    "env.reset()\n",
    "env.board = ['O', 'O', ' ', \n",
    "             ' ', 'X', ' ', \n",
    "             ' ', ' ', ' ']\n",
    "env.current_player = 'X'\n",
    "\n",
    "print(\"Critical position - X must block position 2!\")\n",
    "show_q_values(agent_x, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Improving Training: Self-Play\n",
    "\n",
    "Training against a random opponent is limited. The agent can learn much better by playing against itself (self-play)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_self_play(n_episodes=20000, verbose_every=5000):\n",
    "    \"\"\"Train two agents by playing against each other.\"\"\"\n",
    "    env = TicTacToe()\n",
    "    \n",
    "    # Two agents that learn simultaneously\n",
    "    agent_x = QLearningAgent(player='X', alpha=0.1, gamma=0.9, epsilon=0.3)\n",
    "    agent_o = QLearningAgent(player='O', alpha=0.1, gamma=0.9, epsilon=0.3)\n",
    "    \n",
    "    results = {'X': 0, 'O': 0, 'Draw': 0}\n",
    "    draw_history = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        trajectory_x = []\n",
    "        trajectory_o = []\n",
    "        \n",
    "        while not env.done:\n",
    "            if env.current_player == 'X':\n",
    "                action = agent_x.select_action(env, training=True)\n",
    "                old_state = state\n",
    "                state, reward, done = env.step(action)\n",
    "                trajectory_x.append((old_state, action, reward, state, done))\n",
    "            else:\n",
    "                action = agent_o.select_action(env, training=True)\n",
    "                old_state = state\n",
    "                state, reward, done = env.step(action)\n",
    "                # O gets negative of X's reward\n",
    "                trajectory_o.append((old_state, action, -reward, state, done))\n",
    "        \n",
    "        # Update final rewards based on who won\n",
    "        if env.winner == 'X' and trajectory_o:\n",
    "            s, a, r, ns, d = trajectory_o[-1]\n",
    "            trajectory_o[-1] = (s, a, -1, ns, d)  # O lost\n",
    "        elif env.winner == 'O' and trajectory_x:\n",
    "            s, a, r, ns, d = trajectory_x[-1]\n",
    "            trajectory_x[-1] = (s, a, -1, ns, d)  # X lost\n",
    "        \n",
    "        # Learn from trajectories\n",
    "        for s, a, r, ns, d in trajectory_x:\n",
    "            next_valid = [] if d else [i for i in range(9) if ns[i] == ' ']\n",
    "            agent_x.update(s, a, r, ns, next_valid, d)\n",
    "        \n",
    "        for s, a, r, ns, d in trajectory_o:\n",
    "            next_valid = [] if d else [i for i in range(9) if ns[i] == ' ']\n",
    "            agent_o.update(s, a, r, ns, next_valid, d)\n",
    "        \n",
    "        # Track results\n",
    "        if env.winner == 'X':\n",
    "            results['X'] += 1\n",
    "        elif env.winner == 'O':\n",
    "            results['O'] += 1\n",
    "        else:\n",
    "            results['Draw'] += 1\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            draw_history.append(results['Draw'] / (episode + 1))\n",
    "        \n",
    "        if verbose_every and (episode + 1) % verbose_every == 0:\n",
    "            print(f\"Episode {episode + 1}: X={results['X']}, O={results['O']}, \"\n",
    "                  f\"Draws={results['Draw']} ({results['Draw']/(episode+1)*100:.1f}%)\")\n",
    "    \n",
    "    return agent_x, agent_o, draw_history\n",
    "\n",
    "print(\"Training with self-play...\\n\")\n",
    "agent_x_sp, agent_o_sp, draw_history = train_self_play(n_episodes=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot draw rate over training\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.arange(100, 20001, 100), draw_history)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Draw Rate')\n",
    "plt.title('Self-Play Training: Draw Rate Over Time')\n",
    "plt.axhline(y=1.0, color='g', linestyle='--', label='Optimal (100% draws)', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAs agents improve, they draw more often (optimal play always draws in Tic-Tac-Toe)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate self-play trained agent against random\n",
    "results = evaluate_agent(agent_x_sp, random_agent, n_games=1000)\n",
    "\n",
    "print(\"Self-Play Trained Agent vs Random (1000 games):\")\n",
    "print(f\"  Wins:   {results['wins']} ({results['wins']/10:.1f}%)\")\n",
    "print(f\"  Losses: {results['losses']} ({results['losses']/10:.1f}%)\")\n",
    "print(f\"  Draws:  {results['draws']} ({results['draws']/10:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Effect of Hyperparameters\n",
    "\n",
    "Let's see how different hyperparameters affect learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_train_and_eval(alpha, gamma, epsilon, n_episodes=5000):\n",
    "    \"\"\"Train agent and return final performance.\"\"\"\n",
    "    agent = QLearningAgent(player='X', alpha=alpha, gamma=gamma, epsilon=epsilon)\n",
    "    env = TicTacToe()\n",
    "    \n",
    "    # Quick training against random\n",
    "    for _ in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        trajectory = []\n",
    "        \n",
    "        while not env.done:\n",
    "            if env.current_player == 'X':\n",
    "                action = agent.select_action(env, training=True)\n",
    "                old_state = state\n",
    "                state, reward, done = env.step(action)\n",
    "                trajectory.append((old_state, action, reward, state, done))\n",
    "            else:\n",
    "                action = random_agent(env)\n",
    "                state, reward, done = env.step(action)\n",
    "                if done and trajectory:\n",
    "                    s, a, r, ns, d = trajectory[-1]\n",
    "                    trajectory[-1] = (s, a, -reward, state, True)\n",
    "        \n",
    "        for s, a, r, ns, d in trajectory:\n",
    "            next_valid = [] if d else [i for i in range(9) if ns[i] == ' ']\n",
    "            agent.update(s, a, r, ns, next_valid, d)\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluate_agent(agent, random_agent, n_games=500)\n",
    "    return results['wins'] / 500 * 100\n",
    "\n",
    "# Test different learning rates\n",
    "print(\"Effect of Learning Rate (α):\")\n",
    "alphas = [0.01, 0.1, 0.3, 0.5]\n",
    "for alpha in alphas:\n",
    "    win_rate = quick_train_and_eval(alpha=alpha, gamma=0.9, epsilon=0.2)\n",
    "    print(f\"  α = {alpha}: {win_rate:.1f}% wins\")\n",
    "\n",
    "print(\"\\nEffect of Discount Factor (γ):\")\n",
    "gammas = [0.5, 0.9, 0.99, 1.0]\n",
    "for gamma in gammas:\n",
    "    win_rate = quick_train_and_eval(alpha=0.1, gamma=gamma, epsilon=0.2)\n",
    "    print(f\"  γ = {gamma}: {win_rate:.1f}% wins\")\n",
    "\n",
    "print(\"\\nEffect of Exploration Rate (ε):\")\n",
    "epsilons = [0.0, 0.1, 0.3, 0.5]\n",
    "for epsilon in epsilons:\n",
    "    win_rate = quick_train_and_eval(alpha=0.1, gamma=0.9, epsilon=epsilon)\n",
    "    print(f\"  ε = {epsilon}: {win_rate:.1f}% wins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Key Takeaways\n",
    "\n",
    "### Q-Learning Summary\n",
    "1. **Q-values** estimate the expected future reward for each state-action pair\n",
    "2. **Update rule** moves estimates toward observed rewards + discounted future value\n",
    "3. **Off-policy:** learns about optimal policy while following exploratory policy\n",
    "4. **Tabular:** works for small, discrete state spaces (like Tic-Tac-Toe)\n",
    "\n",
    "### Value-Based vs Policy-Based\n",
    "- **Value-based** (Q-learning): Learn values, derive policy as argmax\n",
    "- **Policy-based** (next week): Learn policy directly\n",
    "- Both have advantages in different settings\n",
    "\n",
    "### Practical Insights\n",
    "- **Self-play** produces stronger agents than training against random\n",
    "- **Hyperparameters matter:** α, γ, ε all affect learning\n",
    "- **Exploration is crucial:** ε=0 often fails to find good strategies\n",
    "- **Evaluation should disable exploration** (ε=0 during testing)\n",
    "\n",
    "### Limitations of Tabular Q-Learning\n",
    "- State space must be finite and small\n",
    "- Can't generalize to unseen states\n",
    "- Next step: **Deep Q-Networks (DQN)** use neural networks to approximate Q-values\n",
    "\n",
    "---\n",
    "\n",
    "## Next: Lab Exercise\n",
    "In the lab, you'll implement Q-learning for the **Nim** game and play against your trained agent!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
