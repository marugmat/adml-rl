{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Introduction to Reinforcement Learning\n",
    "## Lecture Examples\n",
    "\n",
    "**Course:** Reinforcement Learning - Continuing Education  \n",
    "**Institution:** Zurich University of Applied Sciences  \n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the basic RL paradigm: agent, environment, actions, rewards\n",
    "- Grasp the exploration vs exploitation tradeoff\n",
    "- Introduction to Multi-Armed Bandits as the simplest RL problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Reinforcement Learning Paradigm\n",
    "\n",
    "In Reinforcement Learning, an **agent** interacts with an **environment**:\n",
    "1. Agent observes the current **state**\n",
    "2. Agent selects an **action**\n",
    "3. Environment provides a **reward** and new state\n",
    "4. Agent learns to maximize cumulative rewards\n",
    "\n",
    "### Key Difference from Supervised Learning\n",
    "- **Supervised Learning:** Given correct labels, minimize error\n",
    "- **Reinforcement Learning:** Only get reward signals, must discover good actions through trial and error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Armed Bandits: The Simplest RL Problem\n",
    "\n",
    "Imagine a casino with multiple slot machines (\"one-armed bandits\"). Each machine has:\n",
    "- A hidden probability distribution of rewards\n",
    "- Different expected payouts\n",
    "\n",
    "**Your Goal:** Maximize total winnings over many plays\n",
    "\n",
    "**The Challenge:** You don't know which machine is best! You must:\n",
    "- **Explore:** Try different machines to learn about them\n",
    "- **Exploit:** Play the machine you currently think is best\n",
    "\n",
    "This is called the **Exploration-Exploitation Tradeoff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example: A Simple 3-Armed Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBandit:\n",
    "    \"\"\"A simple multi-armed bandit with Gaussian reward distributions.\"\"\"\n",
    "    \n",
    "    def __init__(self, true_means):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            true_means: List of true mean rewards for each arm\n",
    "        \"\"\"\n",
    "        self.true_means = np.array(true_means)\n",
    "        self.n_arms = len(true_means)\n",
    "        \n",
    "    def pull(self, arm):\n",
    "        \"\"\"Pull an arm and get a reward (mean + Gaussian noise).\"\"\"\n",
    "        reward = self.true_means[arm] + np.random.randn()\n",
    "        return reward\n",
    "\n",
    "# Create a bandit with 3 arms\n",
    "# True means: [1.0, 2.5, 1.5] - Arm 1 is the best!\n",
    "bandit = SimpleBandit([1.0, 2.5, 1.5])\n",
    "\n",
    "print(\"True mean rewards:\")\n",
    "for i, mean in enumerate(bandit.true_means):\n",
    "    print(f\"  Arm {i}: {mean:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pull each arm a few times to see the randomness\n",
    "print(\"\\nSample rewards from pulling each arm 5 times:\")\n",
    "for arm in range(3):\n",
    "    rewards = [bandit.pull(arm) for _ in range(5)]\n",
    "    print(f\"Arm {arm}: {[f'{r:.2f}' for r in rewards]}\")\n",
    "    print(f\"  Sample mean: {np.mean(rewards):.2f} (true mean: {bandit.true_means[arm]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Strategy 1: Pure Exploitation (Greedy)\n",
    "\n",
    "Always choose the arm with highest estimated value.\n",
    "\n",
    "**Problem:** What if our initial estimates are wrong? We might get stuck with a suboptimal arm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_strategy(n_steps=1000):\n",
    "    \"\"\"Pure exploitation: always pick the current best arm.\"\"\"\n",
    "    bandit = SimpleBandit([1.0, 2.5, 1.5])\n",
    "    \n",
    "    # Initialize\n",
    "    n_arms = 3\n",
    "    counts = np.zeros(n_arms)  # How many times each arm was pulled\n",
    "    values = np.zeros(n_arms)  # Estimated value of each arm\n",
    "    rewards = []               # Track rewards over time\n",
    "    \n",
    "    # Pull each arm once to initialize\n",
    "    for arm in range(n_arms):\n",
    "        reward = bandit.pull(arm)\n",
    "        counts[arm] += 1\n",
    "        values[arm] = reward\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    # Greedy selection for remaining steps\n",
    "    for step in range(n_arms, n_steps):\n",
    "        # Pick the arm with highest estimated value\n",
    "        arm = np.argmax(values)\n",
    "        \n",
    "        # Pull the arm and observe reward\n",
    "        reward = bandit.pull(arm)\n",
    "        \n",
    "        # Update estimates\n",
    "        counts[arm] += 1\n",
    "        values[arm] += (reward - values[arm]) / counts[arm]  # Incremental average\n",
    "        \n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return rewards, counts, values\n",
    "\n",
    "rewards_greedy, counts_greedy, values_greedy = greedy_strategy(1000)\n",
    "\n",
    "print(\"Greedy Strategy Results:\")\n",
    "print(f\"Average reward: {np.mean(rewards_greedy):.3f}\")\n",
    "print(f\"\\nArm selection counts: {counts_greedy}\")\n",
    "print(f\"Estimated values: {values_greedy}\")\n",
    "print(f\"True values: {bandit.true_means}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Strategy 2: Epsilon-Greedy\n",
    "\n",
    "Balance exploration and exploitation:\n",
    "- With probability ε: **Explore** (pick a random arm)\n",
    "- With probability 1-ε: **Exploit** (pick the best arm)\n",
    "\n",
    "Common values: ε = 0.1 or ε = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_strategy(epsilon=0.1, n_steps=1000):\n",
    "    \"\"\"Epsilon-greedy: explore with probability epsilon, exploit otherwise.\"\"\"\n",
    "    bandit = SimpleBandit([1.0, 2.5, 1.5])\n",
    "    \n",
    "    n_arms = 3\n",
    "    counts = np.zeros(n_arms)\n",
    "    values = np.zeros(n_arms)\n",
    "    rewards = []\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.random() < epsilon:\n",
    "            # Explore: random action\n",
    "            arm = np.random.randint(n_arms)\n",
    "        else:\n",
    "            # Exploit: best action\n",
    "            arm = np.argmax(values)\n",
    "        \n",
    "        # Pull arm and observe reward\n",
    "        reward = bandit.pull(arm)\n",
    "        \n",
    "        # Update estimates\n",
    "        counts[arm] += 1\n",
    "        values[arm] += (reward - values[arm]) / counts[arm]\n",
    "        \n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return rewards, counts, values\n",
    "\n",
    "rewards_eps, counts_eps, values_eps = epsilon_greedy_strategy(epsilon=0.1, n_steps=1000)\n",
    "\n",
    "print(\"Epsilon-Greedy Strategy Results (ε=0.1):\")\n",
    "print(f\"Average reward: {np.mean(rewards_eps):.3f}\")\n",
    "print(f\"\\nArm selection counts: {counts_eps}\")\n",
    "print(f\"Estimated values: {values_eps}\")\n",
    "print(f\"True values: {bandit.true_means}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple experiments\n",
    "n_experiments = 100\n",
    "n_steps = 1000\n",
    "\n",
    "avg_rewards_greedy = np.zeros(n_steps)\n",
    "avg_rewards_eps = np.zeros(n_steps)\n",
    "\n",
    "for _ in range(n_experiments):\n",
    "    rewards_g, _, _ = greedy_strategy(n_steps)\n",
    "    rewards_e, _, _ = epsilon_greedy_strategy(epsilon=0.1, n_steps=n_steps)\n",
    "    \n",
    "    avg_rewards_greedy += rewards_g\n",
    "    avg_rewards_eps += rewards_e\n",
    "\n",
    "avg_rewards_greedy /= n_experiments\n",
    "avg_rewards_eps /= n_experiments\n",
    "\n",
    "# Calculate cumulative averages\n",
    "cumavg_greedy = np.cumsum(avg_rewards_greedy) / np.arange(1, n_steps + 1)\n",
    "cumavg_eps = np.cumsum(avg_rewards_eps) / np.arange(1, n_steps + 1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(avg_rewards_greedy, alpha=0.7, label='Greedy')\n",
    "plt.plot(avg_rewards_eps, alpha=0.7, label='Epsilon-Greedy (ε=0.1)')\n",
    "plt.axhline(y=2.5, color='r', linestyle='--', label='Optimal (Arm 1)', alpha=0.5)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward per Step')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(cumavg_greedy, label='Greedy')\n",
    "plt.plot(cumavg_eps, label='Epsilon-Greedy (ε=0.1)')\n",
    "plt.axhline(y=2.5, color='r', linestyle='--', label='Optimal', alpha=0.5)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Cumulative Average Reward')\n",
    "plt.title('Cumulative Average Reward')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal cumulative average reward:\")\n",
    "print(f\"  Greedy: {cumavg_greedy[-1]:.3f}\")\n",
    "print(f\"  Epsilon-Greedy: {cumavg_eps[-1]:.3f}\")\n",
    "print(f\"  Optimal: 2.500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "1. **RL is about trial-and-error learning** through interaction with an environment\n",
    "2. **Exploration vs Exploitation** is fundamental - we need both!\n",
    "3. **Multi-Armed Bandits** are the simplest RL problem (no state transitions)\n",
    "4. **Greedy strategies** can get stuck with suboptimal choices\n",
    "5. **Epsilon-greedy** provides a simple but effective way to balance exploration and exploitation\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "In the lab assignment, you'll implement more sophisticated exploration strategies and analyze their performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
