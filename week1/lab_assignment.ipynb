{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Lab Assignment - Multi-Armed Bandits\n",
    "\n",
    "**Course:** Reinforcement Learning - Continuing Education  \n",
    "**Institution:** Zurich University of Applied Sciences  \n",
    "**Estimated Time:** 2-3 hours\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "In this lab, you will:\n",
    "1. Implement a multi-armed bandit environment from scratch\n",
    "2. Implement and compare different exploration strategies\n",
    "3. Analyze the exploration-exploitation tradeoff\n",
    "4. Understand the impact of different parameters on learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implement the Bandit Environment (15 minutes)\n",
    "\n",
    "Create a `MultiArmedBandit` class that simulates a bandit problem with multiple arms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    \"\"\"\n",
    "    Multi-Armed Bandit environment.\n",
    "    \n",
    "    Each arm has a Gaussian reward distribution with a specific mean and standard deviation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int, means: List[float] = None, std: float = 1.0):\n",
    "        \"\"\"\n",
    "        Initialize the bandit.\n",
    "        \n",
    "        Args:\n",
    "            n_arms: Number of arms\n",
    "            means: List of mean rewards for each arm. If None, randomly generate.\n",
    "            std: Standard deviation of reward distributions\n",
    "        \"\"\"\n",
    "        self.n_arms = n_arms\n",
    "        self.std = std\n",
    "        \n",
    "        if means is None:\n",
    "            # TODO: Generate random means from a normal distribution N(0, 1)\n",
    "            self.means = None  # Replace with your implementation\n",
    "        else:\n",
    "            self.means = np.array(means)\n",
    "        \n",
    "        # Track which arm is optimal\n",
    "        self.optimal_arm = np.argmax(self.means)\n",
    "    \n",
    "    def pull(self, arm: int) -> float:\n",
    "        \"\"\"\n",
    "        Pull an arm and receive a reward.\n",
    "        \n",
    "        Args:\n",
    "            arm: Index of the arm to pull (0 to n_arms-1)\n",
    "            \n",
    "        Returns:\n",
    "            reward: Reward sampled from the arm's distribution\n",
    "        \"\"\"\n",
    "        # TODO: Return a reward sampled from N(means[arm], std^2)\n",
    "        reward = 0  # Replace with your implementation\n",
    "        return reward\n",
    "    \n",
    "    def get_optimal_value(self) -> float:\n",
    "        \"\"\"Return the expected reward of the optimal arm.\"\"\"\n",
    "        return self.means[self.optimal_arm]\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "bandit = MultiArmedBandit(n_arms=5, means=[0.5, 1.2, 0.8, 2.1, 1.5])\n",
    "print(f\"Bandit means: {bandit.means}\")\n",
    "print(f\"Optimal arm: {bandit.optimal_arm} with mean {bandit.get_optimal_value():.2f}\")\n",
    "print(f\"\\nSample rewards from arm {bandit.optimal_arm}: \", [f\"{bandit.pull(bandit.optimal_arm):.2f}\" for _ in range(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implement Epsilon-Greedy Agent (20 minutes)\n",
    "\n",
    "Create an agent that uses the epsilon-greedy strategy for action selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyAgent:\n",
    "    \"\"\"\n",
    "    Agent using epsilon-greedy action selection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int, epsilon: float = 0.1, initial_value: float = 0.0):\n",
    "        \"\"\"\n",
    "        Initialize the agent.\n",
    "        \n",
    "        Args:\n",
    "            n_arms: Number of arms\n",
    "            epsilon: Exploration probability\n",
    "            initial_value: Initial Q-value estimates for all arms\n",
    "        \"\"\"\n",
    "        self.n_arms = n_arms\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Q-values: estimated value of each arm\n",
    "        self.Q = np.ones(n_arms) * initial_value\n",
    "        \n",
    "        # Count how many times each arm was pulled\n",
    "        self.N = np.zeros(n_arms)\n",
    "    \n",
    "    def select_action(self) -> int:\n",
    "        \"\"\"\n",
    "        Select an action using epsilon-greedy strategy.\n",
    "        \n",
    "        Returns:\n",
    "            action: Selected arm index\n",
    "        \"\"\"\n",
    "        # TODO: Implement epsilon-greedy action selection\n",
    "        # With probability epsilon, choose a random arm\n",
    "        # Otherwise, choose the arm with highest Q-value\n",
    "        # Hint: Use np.random.random() and np.argmax()\n",
    "        \n",
    "        action = 0  # Replace with your implementation\n",
    "        return action\n",
    "    \n",
    "    def update(self, action: int, reward: float):\n",
    "        \"\"\"\n",
    "        Update Q-value estimate after observing a reward.\n",
    "        \n",
    "        Args:\n",
    "            action: The arm that was pulled\n",
    "            reward: The observed reward\n",
    "        \"\"\"\n",
    "        # TODO: Update the Q-value using incremental mean formula:\n",
    "        # Q_new = Q_old + (1/N) * (reward - Q_old)\n",
    "        # where N is the number of times this arm was pulled\n",
    "        \n",
    "        pass  # Replace with your implementation\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "agent = EpsilonGreedyAgent(n_arms=5, epsilon=0.1)\n",
    "print(f\"Initial Q-values: {agent.Q}\")\n",
    "\n",
    "# Simulate a few pulls\n",
    "for _ in range(10):\n",
    "    action = agent.select_action()\n",
    "    reward = np.random.randn() + 1.0  # Dummy reward\n",
    "    agent.update(action, reward)\n",
    "\n",
    "print(f\"\\nAfter 10 steps:\")\n",
    "print(f\"Q-values: {agent.Q}\")\n",
    "print(f\"Arm counts: {agent.N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implement Upper Confidence Bound (UCB) Agent (25 minutes)\n",
    "\n",
    "The UCB algorithm provides a principled way to balance exploration and exploitation.\n",
    "\n",
    "**UCB Action Selection:**\n",
    "$$A_t = \\arg\\max_a \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right]$$\n",
    "\n",
    "Where:\n",
    "- $Q_t(a)$ is the estimated value of action $a$\n",
    "- $N_t(a)$ is the number of times action $a$ was selected\n",
    "- $t$ is the total time step\n",
    "- $c$ is the exploration parameter (typically $c=2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBAgent:\n",
    "    \"\"\"\n",
    "    Agent using Upper Confidence Bound (UCB) action selection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_arms: int, c: float = 2.0, initial_value: float = 0.0):\n",
    "        \"\"\"\n",
    "        Initialize the agent.\n",
    "        \n",
    "        Args:\n",
    "            n_arms: Number of arms\n",
    "            c: Exploration parameter\n",
    "            initial_value: Initial Q-value estimates\n",
    "        \"\"\"\n",
    "        self.n_arms = n_arms\n",
    "        self.c = c\n",
    "        \n",
    "        self.Q = np.ones(n_arms) * initial_value\n",
    "        self.N = np.zeros(n_arms)\n",
    "        self.t = 0  # Total time steps\n",
    "    \n",
    "    def select_action(self) -> int:\n",
    "        \"\"\"\n",
    "        Select an action using UCB.\n",
    "        \n",
    "        Returns:\n",
    "            action: Selected arm index\n",
    "        \"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        # TODO: Implement UCB action selection\n",
    "        # For arms that haven't been pulled yet (N[a] == 0), give them infinite value\n",
    "        # to ensure they are explored first\n",
    "        # For other arms, calculate UCB = Q[a] + c * sqrt(ln(t) / N[a])\n",
    "        # Return the arm with highest UCB value\n",
    "        # Hint: Use np.where() to handle division by zero\n",
    "        \n",
    "        action = 0  # Replace with your implementation\n",
    "        return action\n",
    "    \n",
    "    def update(self, action: int, reward: float):\n",
    "        \"\"\"\n",
    "        Update Q-value estimate.\n",
    "        \n",
    "        Args:\n",
    "            action: The arm that was pulled\n",
    "            reward: The observed reward\n",
    "        \"\"\"\n",
    "        # TODO: Same update rule as epsilon-greedy\n",
    "        pass  # Replace with your implementation\n",
    "\n",
    "\n",
    "# Test your implementation\n",
    "agent = UCBAgent(n_arms=5, c=2.0)\n",
    "print(f\"Initial Q-values: {agent.Q}\")\n",
    "\n",
    "# Simulate a few pulls\n",
    "for _ in range(10):\n",
    "    action = agent.select_action()\n",
    "    reward = np.random.randn() + 1.0\n",
    "    agent.update(action, reward)\n",
    "\n",
    "print(f\"\\nAfter 10 steps:\")\n",
    "print(f\"Q-values: {agent.Q}\")\n",
    "print(f\"Arm counts: {agent.N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Experiment Runner (15 minutes)\n",
    "\n",
    "Create a function to run experiments and collect metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(agent_class, agent_params: dict, bandit: MultiArmedBandit, \n",
    "                   n_steps: int = 1000) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Run a single experiment with a given agent and bandit.\n",
    "    \n",
    "    Args:\n",
    "        agent_class: The agent class to use (EpsilonGreedyAgent or UCBAgent)\n",
    "        agent_params: Dictionary of parameters to pass to agent constructor\n",
    "        bandit: The bandit environment\n",
    "        n_steps: Number of steps to run\n",
    "        \n",
    "    Returns:\n",
    "        rewards: Array of rewards obtained at each step\n",
    "        optimal_actions: Binary array indicating if optimal action was taken\n",
    "        q_values_final: Final Q-value estimates\n",
    "    \"\"\"\n",
    "    # TODO: Implement the experiment runner\n",
    "    # 1. Create the agent\n",
    "    # 2. For each step:\n",
    "    #    - Select an action\n",
    "    #    - Pull the bandit arm\n",
    "    #    - Update the agent\n",
    "    #    - Record the reward\n",
    "    #    - Record if optimal action was taken\n",
    "    # 3. Return collected metrics\n",
    "    \n",
    "    rewards = np.zeros(n_steps)\n",
    "    optimal_actions = np.zeros(n_steps)\n",
    "    \n",
    "    # Your implementation here\n",
    "    \n",
    "    return rewards, optimal_actions, None\n",
    "\n",
    "\n",
    "def run_multiple_experiments(agent_class, agent_params: dict, n_experiments: int = 100, \n",
    "                            n_steps: int = 1000, n_arms: int = 10) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Run multiple experiments and average the results.\n",
    "    \n",
    "    Args:\n",
    "        agent_class: The agent class to use\n",
    "        agent_params: Parameters for the agent\n",
    "        n_experiments: Number of experiments to run\n",
    "        n_steps: Steps per experiment\n",
    "        n_arms: Number of arms in the bandit\n",
    "        \n",
    "    Returns:\n",
    "        avg_rewards: Average rewards across experiments\n",
    "        avg_optimal: Average percentage of optimal actions\n",
    "    \"\"\"\n",
    "    all_rewards = np.zeros((n_experiments, n_steps))\n",
    "    all_optimal = np.zeros((n_experiments, n_steps))\n",
    "    \n",
    "    for i in range(n_experiments):\n",
    "        # Create a new bandit for each experiment\n",
    "        bandit = MultiArmedBandit(n_arms=n_arms)\n",
    "        \n",
    "        rewards, optimal_actions, _ = run_experiment(agent_class, agent_params, bandit, n_steps)\n",
    "        \n",
    "        all_rewards[i] = rewards\n",
    "        all_optimal[i] = optimal_actions\n",
    "    \n",
    "    return np.mean(all_rewards, axis=0), np.mean(all_optimal, axis=0)\n",
    "\n",
    "\n",
    "# Test with a simple case\n",
    "test_bandit = MultiArmedBandit(n_arms=5, means=[0.5, 1.2, 0.8, 2.1, 1.5])\n",
    "rewards, optimal, _ = run_experiment(EpsilonGreedyAgent, {'n_arms': 5, 'epsilon': 0.1}, test_bandit, n_steps=100)\n",
    "print(f\"Test run completed: {len(rewards)} rewards collected\")\n",
    "print(f\"Average reward: {np.mean(rewards):.3f}\")\n",
    "print(f\"Optimal action rate: {np.mean(optimal)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Compare Epsilon-Greedy with Different Epsilon Values (20 minutes)\n",
    "\n",
    "Compare the performance of epsilon-greedy with ε ∈ {0, 0.01, 0.1, 0.3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run experiments for different epsilon values\n",
    "epsilon_values = [0, 0.01, 0.1, 0.3]\n",
    "n_experiments = 100\n",
    "n_steps = 1000\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Your implementation here\n",
    "# For each epsilon value:\n",
    "#   - Run multiple experiments\n",
    "#   - Store average rewards and optimal action rates\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Average reward over time\n",
    "plt.subplot(1, 2, 1)\n",
    "for eps in epsilon_values:\n",
    "    # TODO: Plot the average reward curve for this epsilon\n",
    "    pass\n",
    "\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs Steps')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Optimal action percentage\n",
    "plt.subplot(1, 2, 2)\n",
    "for eps in epsilon_values:\n",
    "    # TODO: Plot the optimal action percentage for this epsilon\n",
    "    pass\n",
    "\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('% Optimal Action')\n",
    "plt.title('Optimal Action Selection Rate')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Compare Epsilon-Greedy vs UCB (25 minutes)\n",
    "\n",
    "Compare epsilon-greedy (ε=0.1) with UCB (c=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run experiments comparing epsilon-greedy and UCB\n",
    "\n",
    "# Run epsilon-greedy\n",
    "rewards_egreedy, optimal_egreedy = None, None  # Replace with your implementation\n",
    "\n",
    "# Run UCB\n",
    "rewards_ucb, optimal_ucb = None, None  # Replace with your implementation\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Average reward\n",
    "plt.subplot(1, 2, 1)\n",
    "# TODO: Plot both strategies\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Epsilon-Greedy vs UCB: Average Reward')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Optimal action rate\n",
    "plt.subplot(1, 2, 2)\n",
    "# TODO: Plot both strategies\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('% Optimal Action')\n",
    "plt.title('Epsilon-Greedy vs UCB: Optimal Action Rate')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final statistics\n",
    "print(f\"\\nFinal Performance (last 100 steps average):\")\n",
    "# TODO: Print average rewards and optimal action rates for both strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Analysis Questions (15 minutes)\n",
    "\n",
    "Answer the following questions based on your experiments:\n",
    "\n",
    "1. **Effect of epsilon:**\n",
    "   - What happens when ε = 0 (pure greedy)?\n",
    "   - What happens when ε is too large (e.g., 0.3)?\n",
    "   - What is the best epsilon value for this problem?\n",
    "\n",
    "2. **Epsilon-Greedy vs UCB:**\n",
    "   - Which strategy converges faster initially?\n",
    "   - Which strategy achieves better long-term performance?\n",
    "   - When would you prefer UCB over epsilon-greedy?\n",
    "\n",
    "3. **Trade-offs:**\n",
    "   - What is the main advantage of epsilon-greedy?\n",
    "   - What is the main advantage of UCB?\n",
    "   - Can you think of scenarios where epsilon-greedy would be preferred?\n",
    "\n",
    "**Write your answers in the markdown cell below:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Answers:\n",
    "\n",
    "1. **Effect of epsilon:**\n",
    "   - ε = 0: \n",
    "   - ε = 0.3: \n",
    "   - Best ε: \n",
    "\n",
    "2. **Epsilon-Greedy vs UCB:**\n",
    "   - Initial convergence: \n",
    "   - Long-term performance: \n",
    "   - When to use UCB: \n",
    "\n",
    "3. **Trade-offs:**\n",
    "   - Epsilon-greedy advantage: \n",
    "   - UCB advantage: \n",
    "   - Scenarios for epsilon-greedy: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge (Optional)\n",
    "\n",
    "If you finish early, try implementing one of these:\n",
    "\n",
    "1. **Decaying Epsilon:** Implement epsilon-greedy where ε decreases over time\n",
    "2. **Optimistic Initial Values:** Test how initial Q-values affect exploration\n",
    "3. **Gradient Bandit:** Implement a policy gradient approach for bandits\n",
    "4. **Non-stationary Bandits:** Modify the bandit so arm means change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your bonus implementation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Please submit:\n",
    "1. This completed notebook with all cells executed\n",
    "2. Your analysis answers in Part 7\n",
    "3. (Optional) Any bonus implementations\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck! Remember:**\n",
    "- Start with the TODO sections in order\n",
    "- Test each component before moving to the next\n",
    "- Don't hesitate to ask for help if you get stuck\n",
    "- The exploration-exploitation tradeoff is at the heart of RL!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
